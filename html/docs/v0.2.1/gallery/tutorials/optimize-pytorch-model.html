
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-406WJTRD8C"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-406WJTRD8C');
    </script>
    
    <title>Optimize PyTorch Model &#8212; Hidet Documentation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Optimize ONNX Model" href="run-onnx-model.html" />
    <link rel="prev" title="Quick Start" href="../getting-started/quick-start.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.svg" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../getting-started/install.html">
   Installation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../getting-started/build-from-source.html">
     Build from source
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting-started/quick-start.html">
   Quick Start
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Optimize PyTorch Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="run-onnx-model.html">
   Optimize ONNX Model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  How-to Guide
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../how-to-guides/add-new-operator/index.html">
   Add New Operator
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../how-to-guides/add-new-operator-compute-definition.html">
     Define Operator Computation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../how-to-guides/add-new-operator-rule-based.html">
     Using Rule-based Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../how-to-guides/add-new-operator-template-based.html">
     Using Template-based Scheduling
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../how-to-guides/add-operator-resolve-rule.html">
   Add Operator Resolve Rule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../how-to-guides/add-subgraph-rewrite-rule.html">
   Add Sub-Graph Rewrite Rule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../how-to-guides/visualize-flow-graph.html">
   Visualize Flow Graph
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Developer Guide
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../developer-guides/contributing.html">
   Contributing
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../developer-guides/hidet-script/index.html">
   Hidet Script
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../developer-guides/hidet-script-dynamic-kernel.html">
     Writing Dynamic kernel
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Notes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../notes/operator-cache.html">
   Operator Cache
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../python_api/index.html">
   Python API
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python_api/root.html">
     hidet
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python_api/option.html">
     hidet.option
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python_api/driver.html">
     hidet.driver
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python_api/cuda.html">
     hidet.cuda
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python_api/tensor.html">
     hidet.Tensor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python_api/data_types.html">
     hidet.dtypes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python_api/ops/index.html">
     hidet.ops
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../python_api/ir/index.html">
     hidet.ir
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../python_api/ir/type.html">
       hidet.ir.type
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../python_api/ir/expr.html">
       hidet.ir.expr
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../python_api/ir/stmt.html">
       hidet.ir.stmt
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../python_api/ir/func.html">
       hidet.ir.func
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../python_api/ir/compute.html">
       hidet.ir.compute
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../python_api/ir/task.html">
       hidet.ir.task
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../python_api/graph/index.html">
     hidet.graph
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../python_api/graph/frontend/index.html">
       hidet.graph.frontend
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
      <label for="toctree-checkbox-7">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../python_api/graph/frontend/onnx.html">
         hidet.graph.frontend.onnx
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../python_api/graph/frontend/torch.html">
         hidet.graph.frontend.torch
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../python_api/graph/transforms/index.html">
       hidet.graph.transforms
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
      <label for="toctree-checkbox-8">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../python_api/graph/transforms/subgraph_rewrite.html">
         Sub-graph Rewrite Pass
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../python_api/graph/transforms/resolve_variant.html">
         Resolve Operator Pass
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python_api/runtime/index.html">
     hidet.runtime
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python_api/utils/index.html">
     hidet.utils
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../python_api/testing/index.html">
     hidet.testing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../genindex.html">
   Index
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            <a href=/netron target=_blank>Customized Netron</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>
<a href="https://github.com/hidet-org/hidet"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="bottom"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/gallery/tutorials/optimize-pytorch-model.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#search-in-a-larger-search-space">
   Search in a larger search space
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-the-correctness">
   Check the correctness
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#operator-configurations">
   Operator configurations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#use-cuda-graph-to-dispatch-kernels">
     Use CUDA Graph to dispatch kernels
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#use-low-precision-data-type">
     Use low-precision data type
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#print-the-input-graph">
     Print the input graph
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Optimize PyTorch Model</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#search-in-a-larger-search-space">
   Search in a larger search space
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-the-correctness">
   Check the correctness
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#operator-configurations">
   Operator configurations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#use-cuda-graph-to-dispatch-kernels">
     Use CUDA Graph to dispatch kernels
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#use-low-precision-data-type">
     Use low-precision data type
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#print-the-input-graph">
     Print the input graph
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-gallery-tutorials-optimize-pytorch-model-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="optimize-pytorch-model">
<span id="sphx-glr-gallery-tutorials-optimize-pytorch-model-py"></span><h1>Optimize PyTorch Model<a class="headerlink" href="#optimize-pytorch-model" title="Permalink to this headline"><span>¶</span></a></h1>
<p>Hidet provides a backend to pytorch dynamo to optimize PyTorch models. To use this backend, you need to specify ‘hidet’
as the backend when calling <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code> such as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># optimize the model with hidet provided backend &#39;hidet&#39;</span>
<span class="n">model_hidet</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;hidet&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="margin admonition note">
<p class="admonition-title">Note</p>
<p>Currently, all the operators in hidet are generated by hidet itself and
there is no dependency on kernel libraries such as cuDNN or cuBLAS. In the future, we might support to lower some
operators to these libraries if they perform better.</p>
</div>
<p>Under the hood, hidet will convert the PyTorch model to hidet’s graph representation and optimize the computation graph
(such as sub-graph rewrite and fusion, constant folding, etc.). After that, each operator will be lowered to hidet’s
scheduling system to generate the final kernel.</p>
<p>Hidet provides some configurations to control the hidet backend of torch dynamo.</p>
<section id="search-in-a-larger-search-space">
<h2>Search in a larger search space<a class="headerlink" href="#search-in-a-larger-search-space" title="Permalink to this headline"><span>¶</span></a></h2>
<p>There are some operators that are compute-intensive and their scheduling is critical to the performance. We usually need
to search in a schedule space to find the best schedule for them to achieve the best performance on given input shapes.
However, searching in a larger schedule space usually takes longer time to optimize the model. By default, hidet will
use their default schedule to generate the kernel for all input shapes. To search in a larger schedule space to get
better performance, you can configure the search space via <a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.search_space" title="hidet.graph.frontend.torch.DynamoConfig.search_space"><code class="xref py py-func docutils literal notranslate"><span class="pre">search_space()</span></code></a>
:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># There are three search spaces:</span>
<span class="c1"># 0 - use default schedule, no search [Default]</span>
<span class="c1"># 1 - search in a small schedule space (usually 1~30 schedules)</span>
<span class="c1"># 2 - search in a large schedule space (usually more than 30 schedules)</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">search_space</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># After configure the search space, you can optimize the model</span>
<span class="n">model_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;hidet&#39;</span><span class="p">)</span>

<span class="c1"># The actual searching happens when you first run the model to know the input shapes</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model_opt</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<p>Please note that the search space we set through <code class="xref py py-func docutils literal notranslate"><span class="pre">set_search_space()</span></code> will be read and
used when we first run the model, instead of when we call <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code>.</p>
</section>
<section id="check-the-correctness">
<h2>Check the correctness<a class="headerlink" href="#check-the-correctness" title="Permalink to this headline"><span>¶</span></a></h2>
<p>It is important to make sure the optimized model is correct. Hidet provides a configuration to print the numerical
difference between the hidet generated operator and the original pytorch operator. You can configure it via
<a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.correctness_report" title="hidet.graph.frontend.torch.DynamoConfig.correctness_report"><code class="xref py py-func docutils literal notranslate"><span class="pre">correctness_report()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># enable the correctness checking</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">correctness_report</span><span class="p">()</span>
</pre></div>
</div>
<p>After enabling the correctness report, every time a new graph is received to compile, hidet will print the numerical
difference using the dummy inputs (for now, torch dynamo does not expose the actual inputs to backends, thus we can
not use the actual inputs). Let’s take the resnet18 model as an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.backends.cudnn</span>
<span class="kn">import</span> <span class="nn">hidet</span>

<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">register_dynamo_backends</span><span class="p">()</span>  <span class="c1"># register hidet backend to torch dynamo</span>

<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">x</span></a> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
    <span class="s1">&#39;pytorch/vision:v0.9.0&#39;</span><span class="p">,</span> <span class="s1">&#39;resnet18&#39;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cuda" title="torch.nn.Module.cuda" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">model</span><span class="o">.</span><span class="n">cuda</span></a><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="k">with</span> <a href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
    <span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">correctness_report</span><span class="p">()</span>
    <span class="n">model_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;hidet&#39;</span><span class="p">)</span>
    <span class="n">model_opt</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">x</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Now, hidet will use the entry_points mechanism to register as a dynamo backend.
Feel free to remove the line `hidet.frontend.torch.register_dynamo_backends()` in your code.
    kind           operator                                                                          dtype    error    attention
--  -------------  --------------------------------------------------------------------------------  -------  -------  -----------
0   placeholder                                                                                      float32  0.0e+00
1   call_module    Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)      float32  0.0e+00
2   call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  1.4e-07
3   call_module    ReLU(inplace=True)                                                                float32  1.4e-07
4   call_module    MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)        float32  1.4e-07
5   call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  1.9e-06
6   call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  1.8e-06
7   call_module    ReLU(inplace=True)                                                                float32  1.4e-06
8   call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  1.1e-06
9   call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  2.1e-06
10  call_function  operator.iadd                                                                     float32  2.4e-06
11  call_module    ReLU(inplace=True)                                                                float32  2.4e-06
12  call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  2.2e-06
13  call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  1.8e-06
14  call_module    ReLU(inplace=True)                                                                float32  1.6e-06
15  call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  9.7e-07
16  call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  2.1e-06
17  call_function  operator.iadd                                                                     float32  3.1e-06
18  call_module    ReLU(inplace=True)                                                                float32  2.5e-06
19  call_module    Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)    float32  1.1e-03  &lt;------
20  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  5.6e-04  &lt;------
21  call_module    ReLU(inplace=True)                                                                float32  5.1e-04  &lt;------
22  call_module    Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  5.4e-04  &lt;------
23  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  7.6e-04  &lt;------
24  call_module    Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)                    float32  1.2e-06
25  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  1.6e-06
26  call_function  operator.iadd                                                                     float32  7.7e-04  &lt;------
27  call_module    ReLU(inplace=True)                                                                float32  6.6e-04  &lt;------
28  call_module    Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  7.4e-04  &lt;------
29  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  5.7e-04  &lt;------
30  call_module    ReLU(inplace=True)                                                                float32  5.7e-04  &lt;------
31  call_module    Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  3.9e-04  &lt;------
32  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  8.3e-04  &lt;------
33  call_function  operator.iadd                                                                     float32  1.0e-03  &lt;------
34  call_module    ReLU(inplace=True)                                                                float32  9.3e-04  &lt;------
35  call_module    Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)   float32  9.2e-04  &lt;------
36  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  7.2e-04  &lt;------
37  call_module    ReLU(inplace=True)                                                                float32  7.2e-04  &lt;------
38  call_module    Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  7.7e-04  &lt;------
39  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  7.1e-04  &lt;------
40  call_module    Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)                   float32  2.3e-04  &lt;------
41  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  3.1e-04  &lt;------
42  call_function  operator.iadd                                                                     float32  7.9e-04  &lt;------
43  call_module    ReLU(inplace=True)                                                                float32  7.9e-04  &lt;------
44  call_module    Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  6.7e-04  &lt;------
45  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  4.3e-04  &lt;------
46  call_module    ReLU(inplace=True)                                                                float32  4.3e-04  &lt;------
47  call_module    Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  3.1e-04  &lt;------
48  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  8.1e-04  &lt;------
49  call_function  operator.iadd                                                                     float32  8.1e-04  &lt;------
50  call_module    ReLU(inplace=True)                                                                float32  8.1e-04  &lt;------
51  call_module    Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)   float32  6.7e-04  &lt;------
52  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  5.2e-04  &lt;------
53  call_module    ReLU(inplace=True)                                                                float32  5.2e-04  &lt;------
54  call_module    Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  2.8e-04  &lt;------
55  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  1.1e-03  &lt;------
56  call_module    Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)                   float32  2.9e-04  &lt;------
57  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  5.7e-04  &lt;------
58  call_function  operator.iadd                                                                     float32  1.0e-03  &lt;------
59  call_module    ReLU(inplace=True)                                                                float32  9.1e-04  &lt;------
60  call_module    Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  5.8e-04  &lt;------
61  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  5.8e-04  &lt;------
62  call_module    ReLU(inplace=True)                                                                float32  5.8e-04  &lt;------
63  call_module    Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  3.1e-04  &lt;------
64  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  3.8e-03  &lt;------
65  call_function  operator.iadd                                                                     float32  3.8e-03  &lt;------
66  call_module    ReLU(inplace=True)                                                                float32  3.8e-03  &lt;------
67  call_module    AdaptiveAvgPool2d(output_size=(1, 1))                                             float32  1.3e-03  &lt;------
68  call_function  torch.flatten                                                                     float32  1.3e-03  &lt;------
69  call_module    Linear(in_features=512, out_features=1000, bias=True)                             float32  1.8e-03  &lt;------
70  output                                                                                           float32  1.8e-03  &lt;------
</pre></div>
</div>
<div class="margin admonition tip">
<p class="admonition-title">Tip</p>
<p>Usually, we can expect:</p>
<ul class="simple">
<li><p>for float32: <span class="math notranslate nohighlight">\(e_h \leq 10^{-5}\)</span>, and</p></li>
<li><p>for float16: <span class="math notranslate nohighlight">\(e_h \leq 10^{-2}\)</span>.</p></li>
</ul>
</div>
<p>The correctness report will print the harmonic mean of the absolute error and relative error for each operator:</p>
<div class="math notranslate nohighlight">
\[e_h = \frac{|actual - expected|}{|expected| + 1} \quad (\frac{1}{e_h} = \frac{1}{e_a} + \frac{1}{e_r})\]</div>
<p>where <span class="math notranslate nohighlight">\(actual\)</span>, <span class="math notranslate nohighlight">\(expected\)</span> are the actual and expected results of the operator, respectively.
The <span class="math notranslate nohighlight">\(e_a\)</span> and <span class="math notranslate nohighlight">\(e_r\)</span> are the absolute error and relative error, respectively. The harmonic mean error is
printed for each operator.</p>
</section>
<section id="operator-configurations">
<h2>Operator configurations<a class="headerlink" href="#operator-configurations" title="Permalink to this headline"><span>¶</span></a></h2>
<section id="use-cuda-graph-to-dispatch-kernels">
<h3>Use CUDA Graph to dispatch kernels<a class="headerlink" href="#use-cuda-graph-to-dispatch-kernels" title="Permalink to this headline"><span>¶</span></a></h3>
<p>Hidet provides a configuration to use CUDA Graph to dispatch kernels. CUDA Graph is a new feature in CUDA 11.0
that allows us to record the kernel dispatches and replay them later. This feature is useful when we want to
dispatch the same kernels multiple times. Hidet will enable CUDA Graph by default. You can disable it via
<a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.use_cuda_graph" title="hidet.graph.frontend.torch.DynamoConfig.use_cuda_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">use_cuda_graph()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># disable CUDA Graph</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">use_cuda_graph</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>in case you want to use PyTorch’s CUDA Graph feature.</p>
</section>
<section id="use-low-precision-data-type">
<h3>Use low-precision data type<a class="headerlink" href="#use-low-precision-data-type" title="Permalink to this headline"><span>¶</span></a></h3>
<p>Hidet provides a configuration to use low-precision data type. By default, hidet will use the same data type as
the original PyTorch model. You can configure it via <a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.use_fp16" title="hidet.graph.frontend.torch.DynamoConfig.use_fp16"><code class="xref py py-func docutils literal notranslate"><span class="pre">use_fp16()</span></code></a> and
<a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.use_fp16_reduction" title="hidet.graph.frontend.torch.DynamoConfig.use_fp16_reduction"><code class="xref py py-func docutils literal notranslate"><span class="pre">use_fp16_reduction()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># automatically transform the model to use float16 data type</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">use_fp16</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># use float16 data type as the accumulate data type in operators with reduction</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">use_fp16_reduction</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>You do not need to change the inputs feed to the model, as hidet will automatically cast the inputs to the
configured data type automatically in the optimized model.</p>
</section>
<section id="print-the-input-graph">
<h3>Print the input graph<a class="headerlink" href="#print-the-input-graph" title="Permalink to this headline"><span>¶</span></a></h3>
<p>If you are interested in the graph that PyTorch dynamo dispatches to hidet backend, you can configure hidet to
print the graph via <a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.print_input_graph" title="hidet.graph.frontend.torch.DynamoConfig.print_input_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">print_input_graph()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># print the input graph</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">print_input_graph</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Because ResNet18 is a neat model without control flow, we can print the input graph to see how PyTorch dynamo
dispatches the model to hidet backend:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <a href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
    <span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">print_input_graph</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">model_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;hidet&#39;</span><span class="p">)</span>
    <span class="n">model_opt</span><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">x</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>opcode         name                        target                                                      args                                             kwargs
-------------  --------------------------  ----------------------------------------------------------  -----------------------------------------------  --------
placeholder    x                           x                                                           ()                                               {}
call_module    self_conv1                  self_conv1                                                  (x,)                                             {}
call_module    self_bn1                    self_bn1                                                    (self_conv1,)                                    {}
call_module    self_relu                   self_relu                                                   (self_bn1,)                                      {}
call_module    self_maxpool                self_maxpool                                                (self_relu,)                                     {}
call_module    self_layer1_0_conv1         self_layer1_0_conv1                                         (self_maxpool,)                                  {}
call_module    self_layer1_0_bn1           self_layer1_0_bn1                                           (self_layer1_0_conv1,)                           {}
call_module    self_layer1_0_relu          self_layer1_0_relu                                          (self_layer1_0_bn1,)                             {}
call_module    self_layer1_0_conv2         self_layer1_0_conv2                                         (self_layer1_0_relu,)                            {}
call_module    self_layer1_0_bn2           self_layer1_0_bn2                                           (self_layer1_0_conv2,)                           {}
call_function  iadd                        &lt;built-in function iadd&gt;                                    (self_layer1_0_bn2, self_maxpool)                {}
call_module    self_layer1_0_relu_1        self_layer1_0_relu                                          (iadd,)                                          {}
call_module    self_layer1_1_conv1         self_layer1_1_conv1                                         (self_layer1_0_relu_1,)                          {}
call_module    self_layer1_1_bn1           self_layer1_1_bn1                                           (self_layer1_1_conv1,)                           {}
call_module    self_layer1_1_relu          self_layer1_1_relu                                          (self_layer1_1_bn1,)                             {}
call_module    self_layer1_1_conv2         self_layer1_1_conv2                                         (self_layer1_1_relu,)                            {}
call_module    self_layer1_1_bn2           self_layer1_1_bn2                                           (self_layer1_1_conv2,)                           {}
call_function  iadd_1                      &lt;built-in function iadd&gt;                                    (self_layer1_1_bn2, self_layer1_0_relu_1)        {}
call_module    self_layer1_1_relu_1        self_layer1_1_relu                                          (iadd_1,)                                        {}
call_module    self_layer2_0_conv1         self_layer2_0_conv1                                         (self_layer1_1_relu_1,)                          {}
call_module    self_layer2_0_bn1           self_layer2_0_bn1                                           (self_layer2_0_conv1,)                           {}
call_module    self_layer2_0_relu          self_layer2_0_relu                                          (self_layer2_0_bn1,)                             {}
call_module    self_layer2_0_conv2         self_layer2_0_conv2                                         (self_layer2_0_relu,)                            {}
call_module    self_layer2_0_bn2           self_layer2_0_bn2                                           (self_layer2_0_conv2,)                           {}
call_module    self_layer2_0_downsample_0  self_layer2_0_downsample_0                                  (self_layer1_1_relu_1,)                          {}
call_module    self_layer2_0_downsample_1  self_layer2_0_downsample_1                                  (self_layer2_0_downsample_0,)                    {}
call_function  iadd_2                      &lt;built-in function iadd&gt;                                    (self_layer2_0_bn2, self_layer2_0_downsample_1)  {}
call_module    self_layer2_0_relu_1        self_layer2_0_relu                                          (iadd_2,)                                        {}
call_module    self_layer2_1_conv1         self_layer2_1_conv1                                         (self_layer2_0_relu_1,)                          {}
call_module    self_layer2_1_bn1           self_layer2_1_bn1                                           (self_layer2_1_conv1,)                           {}
call_module    self_layer2_1_relu          self_layer2_1_relu                                          (self_layer2_1_bn1,)                             {}
call_module    self_layer2_1_conv2         self_layer2_1_conv2                                         (self_layer2_1_relu,)                            {}
call_module    self_layer2_1_bn2           self_layer2_1_bn2                                           (self_layer2_1_conv2,)                           {}
call_function  iadd_3                      &lt;built-in function iadd&gt;                                    (self_layer2_1_bn2, self_layer2_0_relu_1)        {}
call_module    self_layer2_1_relu_1        self_layer2_1_relu                                          (iadd_3,)                                        {}
call_module    self_layer3_0_conv1         self_layer3_0_conv1                                         (self_layer2_1_relu_1,)                          {}
call_module    self_layer3_0_bn1           self_layer3_0_bn1                                           (self_layer3_0_conv1,)                           {}
call_module    self_layer3_0_relu          self_layer3_0_relu                                          (self_layer3_0_bn1,)                             {}
call_module    self_layer3_0_conv2         self_layer3_0_conv2                                         (self_layer3_0_relu,)                            {}
call_module    self_layer3_0_bn2           self_layer3_0_bn2                                           (self_layer3_0_conv2,)                           {}
call_module    self_layer3_0_downsample_0  self_layer3_0_downsample_0                                  (self_layer2_1_relu_1,)                          {}
call_module    self_layer3_0_downsample_1  self_layer3_0_downsample_1                                  (self_layer3_0_downsample_0,)                    {}
call_function  iadd_4                      &lt;built-in function iadd&gt;                                    (self_layer3_0_bn2, self_layer3_0_downsample_1)  {}
call_module    self_layer3_0_relu_1        self_layer3_0_relu                                          (iadd_4,)                                        {}
call_module    self_layer3_1_conv1         self_layer3_1_conv1                                         (self_layer3_0_relu_1,)                          {}
call_module    self_layer3_1_bn1           self_layer3_1_bn1                                           (self_layer3_1_conv1,)                           {}
call_module    self_layer3_1_relu          self_layer3_1_relu                                          (self_layer3_1_bn1,)                             {}
call_module    self_layer3_1_conv2         self_layer3_1_conv2                                         (self_layer3_1_relu,)                            {}
call_module    self_layer3_1_bn2           self_layer3_1_bn2                                           (self_layer3_1_conv2,)                           {}
call_function  iadd_5                      &lt;built-in function iadd&gt;                                    (self_layer3_1_bn2, self_layer3_0_relu_1)        {}
call_module    self_layer3_1_relu_1        self_layer3_1_relu                                          (iadd_5,)                                        {}
call_module    self_layer4_0_conv1         self_layer4_0_conv1                                         (self_layer3_1_relu_1,)                          {}
call_module    self_layer4_0_bn1           self_layer4_0_bn1                                           (self_layer4_0_conv1,)                           {}
call_module    self_layer4_0_relu          self_layer4_0_relu                                          (self_layer4_0_bn1,)                             {}
call_module    self_layer4_0_conv2         self_layer4_0_conv2                                         (self_layer4_0_relu,)                            {}
call_module    self_layer4_0_bn2           self_layer4_0_bn2                                           (self_layer4_0_conv2,)                           {}
call_module    self_layer4_0_downsample_0  self_layer4_0_downsample_0                                  (self_layer3_1_relu_1,)                          {}
call_module    self_layer4_0_downsample_1  self_layer4_0_downsample_1                                  (self_layer4_0_downsample_0,)                    {}
call_function  iadd_6                      &lt;built-in function iadd&gt;                                    (self_layer4_0_bn2, self_layer4_0_downsample_1)  {}
call_module    self_layer4_0_relu_1        self_layer4_0_relu                                          (iadd_6,)                                        {}
call_module    self_layer4_1_conv1         self_layer4_1_conv1                                         (self_layer4_0_relu_1,)                          {}
call_module    self_layer4_1_bn1           self_layer4_1_bn1                                           (self_layer4_1_conv1,)                           {}
call_module    self_layer4_1_relu          self_layer4_1_relu                                          (self_layer4_1_bn1,)                             {}
call_module    self_layer4_1_conv2         self_layer4_1_conv2                                         (self_layer4_1_relu,)                            {}
call_module    self_layer4_1_bn2           self_layer4_1_bn2                                           (self_layer4_1_conv2,)                           {}
call_function  iadd_7                      &lt;built-in function iadd&gt;                                    (self_layer4_1_bn2, self_layer4_0_relu_1)        {}
call_module    self_layer4_1_relu_1        self_layer4_1_relu                                          (iadd_7,)                                        {}
call_module    self_avgpool                self_avgpool                                                (self_layer4_1_relu_1,)                          {}
call_function  flatten                     &lt;built-in method flatten of type object at 0x7f1c088a4840&gt;  (self_avgpool, 1)                                {}
call_module    self_fc                     self_fc                                                     (flatten,)                                       {}
output         output                      output                                                      ((self_fc,),)                                    {}
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  2.318 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-gallery-tutorials-optimize-pytorch-model-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/a824231294cbc050bb47a52e6857032d/optimize-pytorch-model.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">optimize-pytorch-model.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/e571565f59b2a0466491712c8f894b70/optimize-pytorch-model.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">optimize-pytorch-model.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../getting-started/quick-start.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Quick Start</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="run-onnx-model.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Optimize ONNX Model</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Hidet Team<br/>
  
      &copy; Copyright 2022, Hidet Authors.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>