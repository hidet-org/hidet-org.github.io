{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Define Operator Computation\n\n\nEach operator takes a list of input tensors and produces a list of output tensors:\n\n```python\ninputs: List[Tensor]\noutputs: List[Tensor] = operator(inputs)\n```\n<div class=\"alert alert-info\"><h4>Note</h4><p>:class: margin\n\n  Our pioneers [Halide](https://halide-lang.org/) and [Apache TVM](https://tvm.apache.org/) also employ a similar\n  DSL to define the mathematical definition of an operator.</p></div>\n\nThe precise mathematical definition of each operator in Hidet is defined through a domain-specific-language (DSL).\nIn this tutorial, we will show how to define the mathematical definition of a new operator in Hidet using this DSL,\nwhich is defined in the :py:mod:`hidet.ir.compute` module.\n\n\n## Compute Primitives\nThis module provides compute primitives to define the mathematical computation of an operator:\n\n.. py:function:: tensor_input(name: str, dtype: str, shape: List[int])\n    :noindex:\n\n    The :py:func:`~hidet.ir.compute.tensor_input` primitive defines a tensor input by specifying the name hint, scalar\n    data type, and shape of the tensor.\n\n    .. code-block:: python\n      :caption: Examples\n\n      a = tensor_input('a', dtype='float32', shape=[10, 10])\n      b = tensor_input('b', dtype='float32', shape=[])\n      b = tensor_input('data', dtype='float16', shape=[1, 3, 224, 224])\n\n\n.. py:function:: compute(name: str, shape: List[int], fcompute: Callable[[Var,...], Expr])\n    :noindex:\n\n    The :py:func:`~hidet.ir.compute.compute` primitive defines a tensor by specifying\n\n    - the name of the tensor, just a hint for what the tensor represents,\n    - the shape of the tensor, and\n    - a function that maps an index to the expression that computes the value of the tensor at that index.\n\n    The computation of each element of the tensor is *independent* with each other and can be computed in parallel.\n\n    .. code-block:: python\n        :caption: Semantics\n\n        # compute primitive\n        out = compute(\n            name='hint_name',\n            shape=[n1, n2, ..., nk],\n            fcompute=lambda i1, i2, ..., ik: f(i1, i2, ..., ik)\n        )\n\n        # semantics\n        for i1 in range(n1):\n          for i2 in range(n2):\n            ...\n              for ik in range(nk):\n                out[i1, i2, ..., ik] = f(i1, i2, ..., ik)\n\n    .. note::\n        :class: margin\n\n        In the last example, we used an :py:func:`~hidet.ir.expr.if_then_else` expression to define a conditional\n        expression.\n\n    .. code-block:: python\n      :caption: Examples\n\n      # define an input tensor\n      a = tensor_input('a', dtype='float32', shape=[10, 10])\n\n      # example 1: slice the first column of a\n      b = compute('slice', shape=[10], fcompute=lambda i: a[i, 0])\n\n      # example 2: reverse the rows of matrix a\n      c = compute('reverse', shape=[10, 10], fcompute=lambda i, j: a[9 - i, j])\n\n      # example 3: add 1 to the diagonal elements of a\n      from hidet.ir.expr import if_then_else\n      d = compute(\n        name='diag_add',\n        shape=[10, 10],\n        fcompute=lambda i, j: if_then_else(i == j, then_expr=a[i, j] + 1.0, else_expr=a[i, j])\n      )\n\n\n.. py:function:: reduce(shape: List[int], fcompute: Callable[[Var, ...], Expr], reduce_type='sum')\n    :noindex:\n\n    The :py:func:`~hidet.ir.compute.reduce` primitive conducts a reduction operation on a domain with the given shape.\n    It returns a scalar value and can be used in :py:func:`~hidet.ir.compute.compute` primitive.\n\n    .. code-block:: python\n        :caption: Semantics\n\n        # reduce primitive\n        out = reduce(\n            name='hint_name',\n            shape=[n1, n2, ..., nk],\n            fcompute=lambda i1, i2, ..., ik: f(i1, i2, ..., ik)\n            reduce_type='sum' | 'max' | 'min' | 'avg'\n        )\n\n        # semantics\n        values = []\n        for i1 in range(n1):\n          for i2 in range(n2):\n            ...\n              for ik in range(nk):\n                values.append(f(i1, i2, ..., ik))\n        out = reduce_type(values)\n\n    .. code-block:: python\n      :caption: Examples\n\n      # define an input tensor\n      a = tensor_input('a', dtype='float32', shape=[10, 10])\n\n      # example 1: sum all elements of a\n      c = reduce(shape=[10, 10], fcompute=lambda i, j: a[i, j], reduce_type='sum')\n\n      # example 2: sum the first column of a\n      d = reduce(shape=[10], fcompute=lambda i: a[i, 0], reduce_type='sum')\n\n      # example 3: matrix multiplication\n      b = tensor_input('b', dtype='float32', shape=[10, 10])\n      e = compute(\n          name='e',\n          shape=[10, 10],\n          fcompute=lambda i, j: reduce(\n              shape=[10],\n              fcompute=lambda k: a[i, k] * b[k, j],\n              reduce_type='sum'\n          )\n      )\n\n\n\n.. py:function:: arg_reduce(extent: int, fcompute: Callable[[Var], Expr], reduce_type='max')\n    :noindex:\n\n    Similar to :py:func:`~hidet.ir.compute.reduce`, the :py:func:`~hidet.ir.compute.arg_reduce` primitive conducts a\n    reduction operation on a domain with the given extent. The difference is that it returns the index of the element\n    that corresponds to the reduction result, instead of the result itself.\n\n    .. code-block:: python\n        :caption: Semantics\n\n        # arg_reduce primitive\n        out = arg_reduce(extent, fcompute=lambda i: f(i), reduce_type='max' | 'min')\n\n        # semantics\n        values = []\n        for i in range(extent):\n          values.append(f(i))\n        out = index of the max/min value in values\n\n    .. code-block:: python\n        :caption: Examples\n\n        # define an input tensor\n        a = tensor_input('a', dtype='float32', shape=[10, 10])\n\n        # example: find the index of the max element in each row of a\n        b = compute('b', [10], lambda i: arg_reduce(10, lambda j: a[i, j], reduce_type='max'))\n\n\n## Define a Computation Task\nThe computation of each operator can be described as a directed acyclic graph (DAG). The DAG is composed of tensor\nnodes. Both :py:func:`~hidet.ir.compute.tensor_input` and :py:func:`~hidet.ir.compute.compute` primitives create tensor\nnodes. The edges of the DAG are the dependencies between the tensor nodes. Such a DAG is stored in a \n:py:class:`~hidet.ir.task.Task` object. \n\n.. py:class:: Task(name: str, inputs: List[TensorNode], outputs: List[TensorNode])\n    :noindex:\n\nEach task has a name, a list of inputs, and a list of outputs, correspongding to the inputs and outputs of the operator.\nThe following example shows how to create a task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def demo_task():\n    from hidet.ir.compute import tensor_input, compute\n    from hidet.ir.task import Task\n\n    # define the computation DAG through the compute primitives\n    a = tensor_input('a', dtype='float32', shape=[10])\n    b = tensor_input('b', dtype='float32', shape=[10])\n    c = compute('c', [10], lambda i: a[i] + i)\n    d = compute('d', [10], lambda i: c[9 - i])\n    e = compute('e', [10], lambda i: a[i] + b[i])\n\n    # create a task object\n    task = Task(name='task', inputs=[a, b], outputs=[d, e])\n    print(task)\n\n\ndemo_task()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Its computation DAG can be visualized as follows.\n\n.. graphviz::\n  :caption: An example of computation DAG. In this example, there are 5 tensor nodes, where node A and B are inputs\n            and node D and E are outputs. The computation of node C depends on the computation of node A and B.\n\n  digraph {\n      // rankdir=LR;\n      splines=curved;\n      node [\n          shape=box, style=\"rounded\",\n          height=0.4, width=0.6\n      ];\n      graph [style=\"rounded, dashed\"]\n          subgraph cluster_0 {\n              graph [style=\"rounded, dashed\", margin=\"12\"];\n              node [group=0];\n              label=\"Inputs\";\n              a [label=\"A\"];\n              b [label=\"B\"];\n          }\n          subgraph cluster_1 {\n              graph [style=\"rounded, dashed\", labelloc=\"b\", margin=\"15\"];\n              node [group=1];\n              labeljust=\"b\";\n              d [label=\"D\"];\n              e [label=\"E\"];\n              label=\"Outputs\";\n          }\n          c [label=\"C\"];\n          a -> c -> d\n          a -> e\n          b -> e\n  }\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build and Run a Task\nWe provide a driver function :py:func:`hidet.driver.build_task` to build a task into callable function. The\n:py:func:`~hidet.driver.build_task` function does the following steps to lower the task into a callable function:\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>:class: margin\n\n  A scheduler is a function that takes a task as input and returns an scheduled tensor program defined in an IRModule.</p></div>\n\n1. Dispatch the task to a **scheduler** according to the target device and task.\n2. The scheduler lowers the task into a tensor program, defined with :py:class:`~hidet.ir.func.IRModule`.\n3. Lower and optimize the IRModule.\n4. Code generation that translates the IRModule into the target source code (e.g., **source.cu**).\n5. Call compiler (e.g., **nvcc**) to compile the source code into a dynamic library (i.e., **lib.so**).\n6. Load the dynamic library and wrap it to :py:class:`~hidet.runtime.CompiledFunction` that can be directly called.\n\nWe can define the following function to build and run a task.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import List\nimport hidet\nfrom hidet.ir.task import Task\n\n\ndef run_task(task: Task, inputs: List[hidet.Tensor]):\n    \"\"\"Run given task and print inputs and outputs\"\"\"\n    from hidet.runtime import CompiledTask\n\n    # build the task\n    func: CompiledTask = hidet.drivers.build_task(task, target='cpu')\n\n    # run the compiled task\n    outputs = func.run_async(inputs)\n\n    print('Task:', task.name)\n    print('Inputs:')\n    for tensor in inputs:\n        print(tensor)\n    print('Output:')\n    for tensor in outputs:\n        print(tensor)\n    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code shows how to 1) define the computation, 2) define the task, and 3) build and run the task.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p></p></div>\n :class: margin\n\n Please pay attention to the difference between :class:`~hidet.graph.Tensor` and\n :class:`~hidet.ir.compute.TensorNode`. The former is a tensor object that can be used to store data and trace the\n high-level computation graph of a deep learning model. The latter is a tensor node in the domain-specific language\n that is used to describe the computation of a single operator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidet.ir.compute import tensor_input, reduce, compute, arg_reduce, TensorNode\n\n\n\ndef add_example():\n    a: TensorNode = tensor_input(name='a', dtype='float32', shape=[5])\n    b: TensorNode = tensor_input(name='b', dtype='float32', shape=[5])\n    c: TensorNode = compute(name='c', shape=[5], fcompute=lambda i: a[i] + b[i])\n    task = Task(name='add', inputs=[a, b], outputs=[c])\n    run_task(task, [hidet.randn([5]), hidet.randn([5])])\n\n\nadd_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Examples\n\n.. tip::\n  :class: margin\n\n  All the hidet operators are defined in :py:mod:`hidet.graph.ops` submodule. And all of existing operators\n  are defined through the compute primitives described in this tutorial. Feel free to check the source code to learn\n  more about how to define the computation of different operators.\n\nAt last, we show more examples of using the compute primitives to define operator computation.\n\n### ReduceSum\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def reduce_sum_example():\n    a = tensor_input('a', dtype='float32', shape=[4, 3])\n    b = compute(\n        'b',\n        shape=[4],\n        fcompute=lambda i: reduce(shape=[3], fcompute=lambda j: a[i, j], reduce_type='sum'),\n    )\n    task = Task('reduce_sum', inputs=[a], outputs=[b])\n    run_task(task, [hidet.randn([4, 3])])\n\n\nreduce_sum_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ArgMax\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def arg_max_example():\n    a = tensor_input('a', dtype='float32', shape=[4, 3])\n    b = compute(\n        'b',\n        shape=[4],\n        fcompute=lambda i: arg_reduce(extent=3, fcompute=lambda j: a[i, j], reduce_type='max'),\n    )\n    task = Task('arg_max', inputs=[a], outputs=[b])\n    run_task(task, [hidet.randn([4, 3])])\n\n\narg_max_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MatMul\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def matmul_example():\n    a = tensor_input('a', dtype='float32', shape=[3, 3])\n    b = tensor_input('b', dtype='float32', shape=[3, 3])\n    c = compute(\n        'c',\n        shape=[3, 3],\n        fcompute=lambda i, j: reduce(\n            shape=[3], fcompute=lambda k: a[i, k] * b[k, j], reduce_type='sum'\n        ),\n    )\n    task = Task('matmul', inputs=[a, b], outputs=[c])\n    run_task(task, [hidet.randn([3, 3]), hidet.randn([3, 3])])\n\n\nmatmul_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Softmax\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def softmax_example():\n    from hidet.ir.primitives import exp\n\n    a = tensor_input('a', dtype='float32', shape=[3])\n    max_val = reduce(shape=[3], fcompute=lambda i: a[i], reduce_type='max')\n    b = compute('b', shape=[3], fcompute=lambda i: a[i] - max_val)\n    exp_a = compute('exp', shape=[3], fcompute=lambda i: exp(b[i]))\n    exp_sum = reduce(shape=[3], fcompute=lambda i: exp_a[i], reduce_type='sum')\n    softmax = compute('softmax', shape=[3], fcompute=lambda i: exp_a[i] / exp_sum)\n\n    task = Task('softmax', inputs=[a], outputs=[softmax])\n    run_task(task, [hidet.randn([3])])\n\n\nsoftmax_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\nIn this tutorial, we introduced the compute primitives that are used to define the computation of operators in Hidet.\nAfter that, we showed how to wrap the computation DAG into a task and build and run the task. In the next step, we\nwill show you how to use these compute primitives to define new operators in Hidet.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}