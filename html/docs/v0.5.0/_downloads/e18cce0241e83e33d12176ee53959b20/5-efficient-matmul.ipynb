{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# More Efficient Matrix Multiplication\n\nIn this example, we show you how to write a more efficient matrix multiplication kernel on NVIDIA GPU that uses shared\nmemory. For simplicity, we omitted some optimizations like software pipelining (see our `paper`_ for more details).\n\n\n\nFeel free to skip this example if you are not familiar with CUDA programming.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport hidet\nfrom hidet.lang import attrs\nfrom hidet.lang import float32, int32\nfrom hidet.lang import as_tensor_pointer, register_tensor, shared_tensor\nfrom hidet.lang.cuda import threadIdx, blockIdx, syncthreads\nfrom hidet.lang.mapping import spatial, auto_map\nfrom hidet.lang.layout import row_major, local_layout\nfrom hidet.utils.benchmark import benchmark_func\n\n# the hyperparameters of the kernel\nwarps_m, warps_n = 4, 2  # we use 4x2 warps\nwarp_m, warp_n = 2, 2  # each warp repeats 2x2 times\nwarp_map_m, warp_map_n = 2, 16  # each warp has 2x16 threads\nthread_m, thread_n = 4, 4  # each thread repeats 4x4 times\n\n# block_size = (64, 256, 8)\nblock_m_size, block_n_size = (\n    warps_m * warp_m * warp_map_m * thread_m,\n    warps_n * warp_n * warp_map_n * thread_n,\n)\nblock_k_size = 8\nnum_warps = warps_m * warps_n  # 8\nnum_threads = num_warps * 32  # 256\n\nwith hidet.lang.script_module() as script_module:\n\n    @hidet.script\n    def relu(x: float32) -> float32:\n        return x if x > 0.0 else 0.0\n\n    @hidet.script\n    def matmul_relu_kernel(\n        a_ptr: ~float32,\n        b_ptr: ~float32,\n        c_ptr: ~float32,\n        m_size: int32,\n        n_size: int32,\n        k_size: int32,\n    ):\n        attrs.func_name = 'matmul_kernel'\n        attrs.cuda.block_dim = num_threads\n        attrs.cuda.grid_dim = (\n            (m_size + block_m_size - 1) // block_m_size,\n            (n_size + block_n_size - 1) // block_n_size,\n        )\n\n        a = as_tensor_pointer(a_ptr, float32, [m_size, k_size])\n        b = as_tensor_pointer(b_ptr, float32, [k_size, n_size])\n        c = as_tensor_pointer(c_ptr, float32, [m_size, n_size])\n\n        # define tensors in shared memory\n        smem_a = shared_tensor(float32, shape=[block_m_size, block_k_size])\n        smem_b = shared_tensor(float32, shape=[block_k_size, block_n_size])\n\n        # define the accumulation tensor in register\n        regs_c = register_tensor(\n            dtype=float32,\n            # shape will be inferred from the layout automatically,\n            # in this case, the shape is [64, 256]\n            layout=(\n                local_layout(warps_m, warps_n)\n                * row_major(warp_m, warp_n)\n                * local_layout(warp_map_m, warp_map_n)\n                * row_major(thread_m, thread_n)\n            ),\n        )\n\n        # initialize the registers\n        mma_mapping = (\n            spatial(warps_m, warps_n)\n            .repeat(warp_m, warp_n)\n            .spatial(warp_map_m, warp_map_n)\n            .repeat(thread_m, thread_n)\n        )\n        for i, j in mma_mapping.on(threadIdx.x):\n            regs_c[i, j] = 0.0\n\n        # iterate over the k tiles\n        num_k_tiles = (k_size + block_k_size - 1) // block_k_size\n        for k_tile in range(num_k_tiles):\n            # load smem_a [block_m_size, block_k_size] from global memory\n            for i, k in auto_map(block_m_size, block_k_size, workers=num_threads).on(threadIdx.x):\n                global_i, global_k = (i + blockIdx.x * block_m_size, k + k_tile * block_k_size)\n                smem_a[i, k] = (\n                    a[global_i, global_k] if global_i < m_size and global_k < k_size else 0.0\n                )\n\n            # load smem_b [block_k_size, block_n_size] from global memory\n            for k, j in auto_map(block_k_size, block_n_size, workers=num_threads).on(threadIdx.x):\n                global_k, global_j = (k + k_tile * block_k_size, j + blockIdx.y * block_n_size)\n                smem_b[k, j] = (\n                    b[global_k, global_j] if global_k < k_size and global_j < n_size else 0.0\n                )\n\n            # synchronize all threads in the block\n            syncthreads()\n\n            # simt matrix multiply accumulate (mma): regs_c = regs_c + smem_a @ smem_b\n            for i, j in mma_mapping.on(threadIdx.x):\n                for k in range(block_k_size):\n                    regs_c[i, j] += smem_a[i, k] * smem_b[k, j]\n\n            # synchronize all threads in the block\n            syncthreads()\n\n        # store regs_c back to global memory\n        for i, j in mma_mapping.on(threadIdx.x):\n            global_i = i + blockIdx.x * block_m_size\n            global_j = j + blockIdx.y * block_n_size\n            if global_i < m_size and global_j < n_size:\n                c[global_i, global_j] = relu(regs_c[i, j])\n\n\nmodule = script_module.build()\n\n\ndef hidet_matmul_relu(a: torch.Tensor, b: torch.Tensor):\n    m_size, n_size, k_size = a.shape[0], b.shape[1], a.shape[1]\n    c = torch.empty([m_size, n_size], device='cuda')\n    module(a, b, c, m_size, n_size, k_size)\n    return c\n\n\ndef torch_matmul_relu(a: torch.Tensor, b: torch.Tensor):\n    return torch.matmul(a, b).relu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the program with different input sizes. This implementation archives about 30% performance of cuBLAS kernels.\nFor more efficient implementations, please refer to the `ones`_ in hidet package.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for m, n, k in [(1024, 1024, 1024), (256, 256, 256), (32, 32, 32)]:\n    a = torch.randn(m, k, dtype=torch.float32, device='cuda')\n    b = torch.randn(k, n, dtype=torch.float32, device='cuda')\n\n    c1 = hidet_matmul_relu(a, b)\n    c2 = torch_matmul_relu(a, b)\n\n    torch.testing.assert_close(c1, c2, atol=1e-4, rtol=1e-4)\n\n    hidet_latency = benchmark_func(lambda: hidet_matmul_relu(a, b), repeat=50)\n    print(f'{m}x{k}x{n}:')\n    print(' torch: {:.3f} ms'.format(benchmark_func(lambda: torch_matmul_relu(a, b))))\n    print(' hidet: {:.3f} ms'.format(benchmark_func(lambda: hidet_matmul_relu(a, b))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the source code:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(module.source())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}