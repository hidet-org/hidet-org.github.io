{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: hidet\n\n# Optimize ONNX Model\n\nThis tutorial walks through the steps to run a model in [ONNX format](https://onnx.ai/) with Hidet.\nThe ResNet50 onnx model exported from PyTorch model zoo would be used as an example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation of ONNX model\nWe first export the pretrained resnet50 model from torchvision model zoo to an onnx model, using\n:external:func:`torch.onnx.export`. After exporting, there will be a file named ``resnet50.onnx``\nunder current working directory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport torch\n\n# the path to save the onnx model\nonnx_path = './resnet50.onnx'\n\n# load pretrained resnet50 and create a random input\ntorch_model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=True, verbose=False)\ntorch_model = torch_model.cuda().eval()\ntorch_data = torch.randn([1, 3, 224, 224]).cuda()\n\n# export the pytorch model to onnx model 'resnet50.onnx'\ntorch.onnx.export(\n    model=torch_model,\n    args=torch_data,\n    f=onnx_path,\n    input_names=['data'],\n    output_names=['output'],\n    dynamic_axes={'data': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n)\n\nprint('{}: {:.1f} MiB'.format(onnx_path, os.path.getsize(onnx_path) / (2**20)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before going further, we first measure the latency of reset50 directly using PyTorch for inference.\nThe :func:`benchmark_func() <hidet.utils.benchmark_func>` function runs the given function multiple times to\nget the median latency.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidet.utils.benchmark import benchmark_func\n\nprint('PyTorch: {:.3f} ms'.format(benchmark_func(lambda: torch_model(torch_data))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the onnx model with Hidet\nTo run the onnx model, we should first load the model with :func:`hidet.graph.frontend.from_onnx` function by giving\nthe path to the onnx model. This function returns callable object, which applies all operators in the onnx model to\nthe input argument and returns the output tensor(s). The onnx model can be dynamic-shaped (e.g., in this example, the\nbatch size is dynamic).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport hidet\n\n# load onnx model 'resnet50.onnx'\nhidet_onnx_module = hidet.graph.frontend.from_onnx(onnx_path)\n\nprint('Input names:', hidet_onnx_module.input_names)\nprint('Output names: ', hidet_onnx_module.output_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imperatively run the model\nTo run the model, we first create a hidet tensor from torch tensor with :func:`hidet.from_torch`. We directly\ncall ``hidet_onnx_module`` to apply the operators in loaded onnx model to the given input tensor and get the output\ntensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a hidet tensor from pytorch tensor.\ndata: hidet.Tensor = hidet.from_torch(torch_data)\n\n# apply the operators in onnx model to given 'data' input tensor\noutput: hidet.Tensor = hidet_onnx_module(data)\n\n# check the output of hidet with pytorch\ntorch_output = torch_model(torch_data).detach()\nnp.testing.assert_allclose(\n    actual=output.cpu().numpy(), desired=torch_output.cpu().numpy(), rtol=1e-2, atol=1e-2\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trace the model and run\nA more efficient way to run the model is to first trace the execution and get the static computation graph of the deep\nlearning model. We can use :func:`hidet.symbol_like` to create a symbol tensor. We can get the symbol tensor output by\nrunning the model with the symbol tensor as input. The output is a symbol tensor that contains all information of how\nit is derived. We can use :func:`hidet.trace_from` to create the static computation graph from the symbol output\ntensor. In hidet, we use :class:`hidet.graph.FlowGraph` to represent such a computation graph, and it is also the\nbasic unit of graph-level optimizations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "symbol_data = hidet.symbol_like(data)\nsymbol_output = hidet_onnx_module(symbol_data)\ngraph: hidet.FlowGraph = hidet.trace_from(symbol_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can directly call the flow graph to run it. A more efficient way is to create a\nCUDA Graph according to the flow graph and run the CUDA Graph.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>:class: margin\n\n  The [CUDA Graph](https://developer.nvidia.com/blog/cuda-graphs/) is a more efficient\n  way to submit workload to NVIDIA GPU, it eliminates most of the framework-side overhead.</p></div>\n\nWe use :meth:`~hidet.graph.FlowGraph.cuda_graph` method of a :class:`~hidet.graph.FlowGraph` to create a\n:class:`~hidet.cuda.graph.CudaGraph`.\nThen, we use :meth:`~hidet.cuda.graph.CudaGraph.run` method to run the cuda graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def bench_hidet_graph(graph: hidet.FlowGraph):\n    cuda_graph = graph.cuda_graph()\n    (output,) = cuda_graph.run([data])\n    np.testing.assert_allclose(\n        actual=output.cpu().numpy(), desired=torch_output.cpu().numpy(), rtol=5e-2, atol=5e-2\n    )\n    print('  Hidet: {:.3f} ms'.format(benchmark_func(lambda: cuda_graph.run())))\n\n\nbench_hidet_graph(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimize FlowGraph\nTo optimize the model, we set the level of operator schedule space to 2 with :func:`hidet.option.search_space`. We also\nconduct graph level optimizations with :func:`hidet.graph.optimize`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set the search space level for kernel tuning. By default, the search space level is 0, which means no kernel tuning.\n# There are three choices: 0, 1, and 2. The higher the level, the better performance but the longer compilation time.\nhidet.option.search_space(0)\n\n# optimize the flow graph, such as operator fusion\nwith hidet.graph.PassContext() as ctx:\n    ctx.save_graph_instrument('./outs/graphs')\n    graph_opt: hidet.FlowGraph = hidet.graph.optimize(graph)\n\nbench_hidet_graph(graph_opt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we search in space 2, we can have the following numbers on RTX 4090:\n\n```text\nPyTorch: 1.806 ms (eager mode)\n  Hidet: 3.477 ms (no optimization)\n  Hidet: 0.841 ms (optimization and search space 2)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\nHidet is a DNN inference framework that accepts ONNX model. It conducts both graph-level and operator-level\noptimizations. We follow the following steps to run an ONNX model in Hidet:\n\n1. Load the model with :func:`hidet.graph.frontend.from_onnx`.\n2. Run the model with symbolic inputs, and use :func:`hidet.trace_from` to create the :class:`hidet.graph.FlowGraph`.\n3. Create a :class:`hidet.cuda.graph.CudaGraph` using :func:`hidet.graph.FlowGraph.cuda_graph`.\n4. Run the cuda graph.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}