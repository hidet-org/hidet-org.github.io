{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Add PyTorch Operator Mapping\n\nThis guide describes how to add an operator mapping for PyTorch.\n\n.. graphviz::\n  :caption: The workflow of hidet backend of :code:`torch.compile(..., backend='hidet')`.\n\n  digraph {\n      // rankdir=LR;\n      splines=curved;\n      node [\n          shape=box, style=\"rounded, filled\",\n          height=0.4, width=0.6, margin=\"0.2,0.10\",\n          fillcolor=\"#EEF0E5\",\n          color=\"#163020\",\n          fontcolor=\"#163020\",\n      ];\n      edge [\n          color=\"#163020\",\n          fontcolor=\"#163020\",\n      ];\n\n\n      graph [style=\"rounded, dashed\"]\n          a [label=\"PyTorch nn.Module\"];\n          b [label=\"torch.fx.Graph\"];\n          c [label=\"hidet.FlowGraph\"];\n          d [label=\"hidet.runtime.CompiledGraph\"];\n\n          a -> b [label=\"   Step 1: PyTorch Dynamo\"];\n          b -> c [label=\"   Step 2: Operator mapping\"];\n          c -> d [label=\"   Step 3: FlowGraph building\"];\n  }\n\nDuring step 2, we convert each pytorch operator to a hidet operator. In a `torch.fx.Graph`, there are three kinds of\noperators that need to be converted:\n\n1. functions (e.g., :code:`torch.nn.functional.relu`, :code:`torch.relu`, :code:`operator.add`, etc.)\n2. modules (e.g., :code:`torch.nn.ReLU`, :code:`torch.nn.Linear`, etc.)\n3. tensor methods (e.g., :code:`torch.Tensor.squeeze`, :code:`torch.Tensor.to`, etc.)\n\nIn this guide, we will show how to add the operator mapping for all the three kinds of operators.\n\n## 1. Prepare Environment\nFirst, we remove some existing operator mapping (i.e., conversion) rules for demonstration purpose, and define an\nexample model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import operator\nimport torch\nfrom torch import nn\n\n# hidet employs an interpreter to convert a fx.Graph to FlowGraph\nfrom hidet.graph.frontend.torch.registry import Registry\n\n# the following three modules register the conversion rules\nimport hidet.graph.frontend.torch.register_functions\nimport hidet.graph.frontend.torch.register_modules\nimport hidet.graph.frontend.torch.register_methods\n\n# Before removing registered functions, make sure to\n# call allow_in_graph_registered_funcs_only() by importing dynamo_backends\nimport hidet.graph.frontend.torch.dynamo_backends\n\n# we remove the rules for the following operators for demonstration purpose\n# we will add them back later\ndel Registry.registered_functions[torch.nn.functional.relu]\ndel Registry.registered_functions[operator.add]\ndel Registry.registered_modules[torch.nn.Linear]\ndel Registry.registered_methods[torch.Tensor.flatten]\n\n\nclass Model(nn.Module):\n    \"\"\"a model used nn.Linear, nn.functional.relu, operator.add and Tensor.flatten\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(10, 10)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = torch.nn.functional.relu(x)\n        x = x + x\n        return x.flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Compile and Run the Model\nIf we compile and run the model, we will get an error that complains about the missing conversion rules for\n:code:`torch.nn.Linear`, :code:`torch.nn.functional.relu` and :code:`operator.add`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_model():\n    model = Model().cuda()\n    model_opt = torch.compile(model, backend='hidet', mode='max-autotune')\n\n    x = torch.randn(10, 10, device='cuda')\n    y1 = model_opt(x)\n    y2 = model(x)\n    torch.testing.assert_close(actual=y1, expected=y2, atol=3e-3, rtol=3e-3)\n    print('success!')\n\n\ntry:\n    run_model()\nexcept Exception as e:\n    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Add Operator Mappings\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Optional\nfrom hidet import ops\nfrom hidet import Tensor\nfrom hidet.graph.frontend.torch.registry import (\n    register_function,\n    register_module,\n    register_method,\n    HidetModule,\n)\n\n\n# register the conversion rule for torch.nn.functional.relu\n@register_function(torch.nn.functional.relu)\ndef torch_relu(x: Tensor, inplace: bool = False):  # the signature must match the original function\n    # the parameter `x` is hidet.Tensor instead of torch.Tensor\n    # we also need to return a hidet.Tensor instead of torch.Tensor\n    _ = inplace  # ignore inplace\n    return ops.relu(x)\n\n\n@register_function(operator.add)\ndef operator_add(x: Tensor, y: Tensor):\n    return ops.add(x, y)\n\n\n@register_module(torch.nn.Linear)\nclass HidetLinear(\n    HidetModule\n):  # HidetModule is a tool class that helps us to convert a torch.nn.Module\n    def __init__(self, torch_module: torch.nn.Module):\n        super().__init__(torch_module)\n        # inside the class, we can access the parameter of the torch module via\n        # `self.param(name: str, optional: bool = False) -> Tensor`\n        # and the returned tensor is a hidet.Tensor\n        self.transposed_weight: Tensor = ops.transpose(self.param('weight'), [1, 0])\n        self.bias: Optional[Tensor] = self.param('bias', optional=True)\n\n    def __call__(self, x: Tensor) -> Tensor:\n        # similarly, the parameter `x` is hidet.Tensor instead of torch.Tensor\n        y = ops.matmul(x, self.transposed_weight)\n        if self.bias is not None:\n            y = y + self.bias\n        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we run the model again, it will complain about the missing conversion rule for :code:`torch.Tensor.flatten`.\nIt does not complain about missing conversion rule for :code:`torch.Tensor.flatten` before because we can not\nknow the type of the method's class (i.e., :code:`torch.Tensor`) before we actually run the model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    run_model()\nexcept Exception as e:\n    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thus, we need to add the conversion rule for :code:`torch.Tensor.flatten` later as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@register_method(torch.Tensor.flatten)\ndef tensor_flatten(self: Tensor, start_dim=0, end_dim=-1):\n    return ops.flatten(self, start_dim=start_dim, end_dim=end_dim)\n\n\nrun_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We put all the registration code in the following three modules:\n\n1. :code:`hidet.graph.frontend.torch.register_functions` (all the functions in `torch.nn.functional.*` and\n   `operator.*`)\n2. :code:`hidet.graph.frontend.torch.register_modules` (all the modules in `torch.nn.*`)\n3. :code:`hidet.graph.frontend.torch.register_methods` (all the methods in `torch.Tensor.*`)\n\nLots of operators have already been registered in the above three modules, and they are also good examples for us\nto learn how to add operator mapping.\n\nUsually, we will use the existing operators in hidet (defined in `hidet.ops.*`) to implement the pytorch operators.\nIf there are no corresponding operators in hidet, we can add the missing operators to `hidet.ops.*` by following the\nguide :doc:`/how-to-guides/add-new-operator/index`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The operator mapping rules are registered in the global registry. Thus, if we register the same operator mapping\n   rules multiple times, only the last registration will take effect.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Summary\nIn this guide, we show how to add operator mapping for PyTorch. We first remove some existing operator mapping rules\nfor demonstration purpose, and then add them back. We also show how to add operator mapping for functions, modules\nand tensor methods.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}