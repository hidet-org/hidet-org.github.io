{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using Template-based Scheduling\n\nIn the previous tutorial, we have learned how to define a new operator with rule-based scheduling. Rule-based scheduling\nis a convenient way to define a new operator, but it is not efficient enough for operators with large amount of\nreduction. In this tutorial, we will learn how to define a new operator with **template-based scheduling**.\nTemplate-based scheduling allows us to define a tensor program template, and the template will be instantiated for\ndifferent input shapes and tunable hyper-parameters.\n\n## Override ``implement_cuda()`` method\nThe :class:`~hidet.ir.task.Task` class have two methods :code:`implement_cpu()` and :code:`implement_cuda()` that\ncan be override when we define a new task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import hidet\nfrom hidet.ir.compute import TensorNode, compute, reduce\nfrom hidet.ir.task import Task\nfrom hidet.ir.module import IRModule\n\n\nclass BatchMatmulFp16Task(Task):\n    def __init__(self, a: TensorNode, b: TensorNode):\n        batch_size, m_size, k_size = a.shape\n        batch_size, k_size, n_size = b.shape\n        c = compute(\n            name='c',\n            shape=[batch_size, m_size, n_size],\n            fcompute=lambda p, i, j: reduce(\n                shape=[k_size], fcompute=lambda k: a[p, i, k] * b[p, k, j], reduce_type='sum'\n            ),\n        )\n        super().__init__(\n            name='batch_matmul_fp16',\n            inputs=[a, b],\n            outputs=[c],\n            attributes={\n                'batch_size': batch_size,\n                'm_size': m_size,\n                'n_size': n_size,\n                'k_size': k_size,\n            },\n        )\n\n    def allow_epilogue(self) -> bool:\n        return False\n\n    def implement_cuda(self, working_dir: str) -> IRModule:\n        # override this method to use template-based scheduling\n        return batch_matmul_mma_fp16_schedule(self)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In above task definition, we override the :code:`implement_cuda()` method to use template-based scheduling. Inside\nthe :code:`implement_cuda()` method, we call the :code:`batch_matmul_mma_fp16_schedule()` function to get a tensor\nprogram that implements the computation defined in the task.\n\n## Implement the tensor-program\nWe can implement the :code:`batch_matmul_mma_fp16_schedule()` function in the following way. This function is\ncomplicated. To learn what it does, we should know both CUDA programming and Hidet Script. Feel free to skip it for\nnow.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>:class: margin\n\n  This function defines the tensor program based on *Hidet Script*. Hidet Script is another domain-specific language\n  in Hidet that allows developers to write tensor programs in python syntax. We will add more documentation\n  to introduce Hidet Script in the future when it gets more stable.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def batch_matmul_mma_fp16_schedule(task: BatchMatmulFp16Task) -> IRModule:\n    from hidet.lang import (\n        f16,\n        spatial,\n        repeat,\n        shared_tensor,\n        register_tensor,\n        attrs,\n        grid,\n        printf,\n        cast,\n    )\n    from hidet.lang.mapping import repeat, spatial\n    from hidet.lang.cuda import blockIdx, threadIdx, syncthreads\n    from hidet.lang.cuda import MmaConfig, mma_sync\n\n    # get the workload size\n    bs = task.attrs['batch_size']\n    m_size = task.attrs['m_size']\n    n_size = task.attrs['n_size']\n    k_size = task.attrs['k_size']\n\n    # define the template hyper-parameters\n    mma_config = MmaConfig.m16n8k8_f16_f16()\n    block_m, block_n, block_k = 128, 128, 8\n    warp_m, warp_n, warp_k = 64, 64, 8\n    warp_count_m, warp_count_n, warp_count_k = 2, 2, 1\n    mma_m, mma_n, mma_k = mma_config.m, mma_config.n, mma_config.k  # 16, 8, 8\n    mma_count_m, mma_count_n, mma_count = 4, 8, 1\n    threads = warp_count_m * warp_count_n * warp_count_k * 32\n\n    # define the tensor program\n    with hidet.script_module() as module:\n\n        @hidet.script\n        def load_regs_a(smem_a: f16[block_m, block_k], regs_a: f16[4, mma_config.a_elements]):\n            \"\"\"Load A registers from shared memory.\"\"\"\n            warp_id, lane_id = threadIdx.x / 32, threadIdx.x % 32\n            for wi, wj, wk in spatial(warp_count_m, warp_count_n, warp_count_k).on(warp_id):\n                for mi in range(mma_count_m):\n                    p = 0\n                    for i, k in mma_config.a_load_map.on(lane_id):\n                        regs_a[mi, p] = smem_a[wi * warp_m + mi * mma_m + i, wk * warp_k + k]\n                        p += 1\n\n        @hidet.script\n        def load_regs_b(smem_b: f16[block_k, block_n], regs_b: f16[8, mma_config.b_elements]):\n            \"\"\"Load B registers from shared memory.\"\"\"\n            warp_id, lane_id = threadIdx.x / 32, threadIdx.x % 32\n            for wi, wj, wk in spatial(warp_count_m, warp_count_n, warp_count_k).on(warp_id):\n                for mj in range(mma_count_n):\n                    p = 0\n                    for k, j in mma_config.b_load_map.on(lane_id):\n                        regs_b[mj, p] = smem_b[wk * warp_k + k, wj * warp_n + mj * mma_n + j]\n                        p += 1\n\n        @hidet.script\n        def warp_mma(\n            regs_a: f16[4, mma_config.a_elements],\n            regs_b: f16[8, mma_config.b_elements],\n            regs_c: f16[4, 8, mma_config.c_elements],\n        ):\n            \"\"\"Perform warp-level matrix multiplication.\"\"\"\n            for mi, mj in repeat(mma_count_m, mma_count_n).on(0):\n                mma_sync(mma_config, ~regs_a[mi, 0], ~regs_b[mj, 0], ~regs_c[mi, mj, 0])\n\n        @hidet.script\n        def store_c(regs_c: f16[4, 8, mma_config.c_elements], c: f16[bs, m_size, n_size]):\n            \"\"\"Store C registers to global memory.\"\"\"\n            warp_id, lane_id = threadIdx.x / 32, threadIdx.x % 32\n            offset_m, offset_n = blockIdx.x * block_m, blockIdx.y * block_n\n            gmem_c = c[blockIdx.z, offset_m:, offset_n:]\n            for k_round in range(warp_count_k):\n                for wi, wj, wk in spatial(warp_count_m, warp_count_n, warp_count_k).on(warp_id):\n                    if wk == k_round:\n                        for mi, mj in repeat(mma_count_m, mma_count_n).on(0):\n                            p = 0\n                            for i, j in mma_config.c_store_map.on(lane_id):\n                                gmem_c.write(\n                                    [wi * warp_m + mi * mma_m + i, wj * warp_n + mj * mma_n + j],\n                                    regs_c[mi, mj, p],\n                                    protected=True,\n                                )\n                                p += 1\n\n        @hidet.script\n        def batch_matmul_kernel(\n            a: f16[bs, m_size, k_size], b: f16[bs, k_size, n_size], c: f16[bs, m_size, n_size]\n        ):\n            \"\"\"Batch matrix multiplication kernel.\"\"\"\n            attrs.cuda.grid_dim = (\n                (m_size + block_m - 1) // block_m,\n                (n_size + block_n - 1) // block_n,\n                bs,\n            )\n            attrs.cuda.block_dim = threads\n            offset_m, offset_n = blockIdx.x * block_m, blockIdx.y * block_n\n            smem_a = shared_tensor('float16', [block_m, block_k])\n            smem_b = shared_tensor('float16', [block_k, block_n])\n            regs_a = register_tensor('float16', [4, mma_config.a_elements])\n            regs_b = register_tensor('float16', [8, mma_config.b_elements])\n            regs_c = register_tensor('float16', [4, 8, mma_config.c_elements])\n\n            for i, j, p in grid(4, 8, mma_config.c_elements):\n                regs_c[i, j, p] = 0.0\n\n            for k0 in range((k_size + block_k - 1) // block_k):\n                offset_k = k0 * block_k\n                gmem_a = a[blockIdx.z, offset_m:, offset_k:]\n                gmem_b = b[blockIdx.z, offset_k:, offset_n:]\n                for i, k in repeat(8, 1).spatial(16, 8).on(threadIdx.x):\n                    smem_a[i, k] = gmem_a.read([i, k], protected=True)\n                for k, j in repeat(8, 1).spatial(1, 128).on(threadIdx.x):\n                    smem_b[k, j] = gmem_b.read([k, j], protected=True)\n                syncthreads()\n                load_regs_a(smem_a, regs_a)\n                load_regs_b(smem_b, regs_b)\n                warp_mma(regs_a, regs_b, regs_c)\n                syncthreads()\n            store_c(regs_c, c)\n\n    ir_module = module.ir_module()\n    return ir_module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the operator\nThe remaining part is the same as the rule-based scheduling method to add new operator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidet.graph import Operator, Tensor\nfrom hidet.graph.ops.utils import input_like\n\n\nclass BatchMatmulFp16Op(Operator):\n    def __init__(self, a: Tensor, b: Tensor):\n        assert a.dtype == hidet.float16 and b.dtype == hidet.float16\n        super().__init__(\n            inputs=[a, b],\n            attributes={},\n            task=BatchMatmulFp16Task(input_like(a, 'a'), input_like(b, 'b')),\n        )\n\n\ndef batch_matmul_fp16(a: Tensor, b: Tensor) -> Tensor:\n    return BatchMatmulFp16Op(a, b).outputs[0]\n\n\ndef demo_usage():\n    a = hidet.randn([1, 2, 2], dtype='float16', device='cuda')\n    b = hidet.randn([1, 2, 2], dtype='float16', device='cuda')\n    c = batch_matmul_fp16(a, b)\n    print(a)\n    print(b)\n    print(c)\n\n\ndemo_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\nIn this tutorial, we have shown how to use the template-based scheduling mechanism to add new operators. Basically,\nwhat we need to do is to override the **implement_cuda** or **implement_cpu** method of the task class, and implement\nthe task to get an IR module. In this example, we used Hidet Script to implement the task, but you can also use\nother ways such as IR builder.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}