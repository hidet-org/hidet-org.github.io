{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: hidet\n\n# Quick Start\n\nThis guide walks through the key functionality of Hidet for tensor computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimize PyTorch model with Hidet\n<div class=\"alert alert-info\"><h4>Note</h4><p>:class: margin\n\n  Torch dynamo is a feature introduced in PyTorch 2.0, which has not been officially released yet. Please install the\n  nightly build of PyTorch to use this feature.</p></div>\n\nThe easiest way to use Hidet is to use the :func:`torch.compile` function with 'hidet' as the backend, such as\n\n```python\nmodel_opt = torch.compile(model, backend='hidet')\n```\nNext, we use resnet18 model as an example to show how to optimize a PyTorch model with Hidet.\n\n.. tip::\n  :class: margin\n\n  Because tf32 is enabled by default for torch's cudnn backend, the torch's precision is slightly low.\n  You could disable the tf32 via ``torch.backends.cudnn.allow_tf32 = False``. See also `PyTorch TF32`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import hidet\nimport torch\n\n# take resnet18 as an example\nx = torch.randn(1, 3, 224, 224).cuda()\nmodel = torch.hub.load(\n    'pytorch/vision:v0.9.0', 'resnet18', pretrained=True, verbose=False\n)\nmodel = model.cuda().eval()\n\n# optimize the model with 'hidet' backend\nmodel_opt = torch.compile(model, backend='hidet')\n\n# run the optimized model\ny1 = model_opt(x)\ny2 = model(x)\n\n# check the correctness\ntorch.testing.assert_close(actual=y1, expected=y2, rtol=1e-2, atol=1e-2)\n\n\n# benchmark the performance\nfor name, model in [('eager', model), ('hidet', model_opt)]:\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n    torch.cuda.synchronize()\n    start_event.record()\n    for _ in range(100):\n        y = model(x)\n    end_event.record()\n    torch.cuda.synchronize()\n    print('{:>10}: {:.3f} ms'.format(name, start_event.elapsed_time(end_event) / 100.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hidet provides some configurations to control the optimization of hidet backend. such as\n\n- **Search Space**: you can choose the search space of operator kernel tuning. A larger schedule space usually\n  achieves the better performance, but takes longer time to optimize.\n- **Correctness Checking**: print the correctness checking report. You can know the numerical difference between the\n  hidet generated operator and the original pytorch operator.\n- **Other Configurations**: you can also configure the other optimizations of hidet backend, such as using a lower\n  precision of data type automatically (e.g., float16), or control the behavior of parallelization of the reduction\n  dimension of the matrix multiplication and convolution operators.\n\n.. seealso::\n\n  You can learn more about the configuration of hidet as a backend in torch dynamo in the tutorial\n  :doc:`/gallery/tutorials/optimize-pytorch-model`.\n\nIn the remaining parts, we will show you the key components of Hidet.\n\n\n## Define tensors\n\n.. tip::\n  :class: margin\n\n  Besides :func:`~hidet.randn`, we can also use :func:`~hidet.zeros`, :func:`~hidet.ones`, :func:`~hidet.full`,\n  :func:`~hidet.empty` to create tensors with different initialized values. We can use :func:`~hidet.from_torch` to\n  convert a PyTorch tensor to Hidet tensor that shares the same memory. We can also use :func:`~hidet.asarray` to\n  convert python list or numpy ndarray to Hidet tensor.\n\nA *tensor* is a n-dimension array. As other machine learning framework,\nHidet takes :class:`~hidet.Tensor` as the core object to compute and manipulate.\nThe following code defines a tensor with randomly initialized tensor with :func:`hidet.randn`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = hidet.randn([2, 3], device='cuda')\nprint(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each :class:`~hidet.Tensor` has :attr:`~hidet.Tensor.dtype` to define the type of each tensor element,\nand :attr:`~hidet.Tensor.device` to tell which device this tensor resides on, and\n:attr:`~hidet.Tensor.shape` to indicate the size of each dimension. The example defines a ``float32`` tensor on\n``cuda`` device with shape ``[2, 3]``.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run operators\nHidet provides :mod:`a bunch of operators <hidet.ops>` (e.g., :func:`~hidet.ops.matmul` and\n:func:`~hidet.ops.conv2d`) to compute and manipulate tensors. We can do a matrix multiplication as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "b = hidet.randn([3, 2], device='cuda')\nc = hidet.randn([2], device='cuda')\nd = hidet.ops.matmul(a, b)\nd = d + c  # 'd + c' is equivalent to 'hidet.ops.add(d, c)'\nprint(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, the operator is executed on the device at the time we call it, thus it is in an `imperative` style\nof execution. Imperative execution is intuitive and easy to debug. But it prevents some graph-level optimization\nopportunities and suffers from higher kernel dispatch latency.\n\nIn the next section, we would introduce another way to execute operators.\n\n## Symbolic tensor and flow graph\nIn hidet, each tensor has an optional :attr:`~hidet.Tensor.storage` attribute that represents a block of\nmemory that stores the contents of the tensor. If the storage attribute is None, the tensor is a `symbolic` tensor.\nWe could use :func:`hidet.symbol_like` or :func:`hidet.symbol` to create a symbolic tensor. Symbolic tensors are\nreturned if any input tensor of an operator is symbolic. We could know how the symbolic tensor is computed via the\n:attr:`~hidet.Tensor.trace` attribute. It is a tuple ``(op, idx)`` where ``op`` is the operator produces this\ntensor and ``idx`` is the index of this tensor in the operator's outputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def linear_bias(x, b, c):\n    return hidet.ops.matmul(x, b) + c\n\n\nx = hidet.symbol_like(a)\ny = linear_bias(x, b, c)\n\nassert x.trace is None\nassert y.trace is not None\n\nprint('x:', x)\nprint('y:', y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use trace attribute to construct the computation graph, starting from the symbolic output tensor(s).\nThis is what function :func:`hidet.trace_from` does. In hidet, we use :class:`hidet.graph.FlowGraph` to\nrepresent the data flow graph (a.k.a, computation graph).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph: hidet.FlowGraph = hidet.trace_from(y)\nprint(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimize flow graph\n.. tip::\n  :class: margin\n\n  We may config optimizations with :class:`~hidet.graph.PassContext`.\n  Potential configs:\n\n  - Whether to use tensor core.\n  - Whether to use low-precision data type (e.g., ``float16``).\n\nFlow graph is the basic unit of graph-level optimizations in hidet. We can optimize a flow graph with\n:func:`hidet.graph.optimize`. This function applies the predefined passes to optimize given flow graph.\nIn this example, we fused the matrix multiplication and element-wise addition into a single operator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt_graph: hidet.FlowGraph = hidet.graph.optimize(graph)\nprint(opt_graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run flow graph\nWe can directly call the flow graph to run it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y1 = opt_graph(a)\nprint(y1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For CUDA device, a more efficient way is to create a cuda graph to dispatch the kernels in a flow graph\nto the NVIDIA GPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cuda_graph = opt_graph.cuda_graph()\noutputs = cuda_graph.run([a])\ny2 = outputs[0]\nprint(y2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\nIn this quick start guide, we walk through several important functionalities of hidet:\n\n- Define tensors.\n- Run operators imperatively.\n- Use symbolic tensor to create computation graph (e.g., flow graph).\n- Optimize and run flow graph.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Step\nIt is time to learn how to use hidet in your project. A good start is to `Run ONNX Model with Hidet`.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}