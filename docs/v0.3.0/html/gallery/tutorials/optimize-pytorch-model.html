

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-406WJTRD8C"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-406WJTRD8C');
    </script>
    
    <title>Optimize PyTorch Model &#8212; Hidet Documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'gallery/tutorials/optimize-pytorch-model';</script>
    <link rel="icon" href="../../_static/favicon.svg"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Optimize ONNX Model" href="optimize-onnx-model.html" />
    <link rel="prev" title="Quick Start" href="../getting-started/quick-start.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.svg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.svg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../getting-started/install.html">Installation</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../getting-started/build-from-source.html">Build from source</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started/quick-start.html">Quick Start</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Optimize PyTorch Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize-onnx-model.html">Optimize ONNX Model</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Hidet Script</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../hidet-script/index.html">Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../hidet-script/examples/index.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../hidet-script/0-hello-world.html">Hello World!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../hidet-script/1-scalar-addition.html">Scalar Addition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../hidet-script/2-vector-addition.html">Vector Addition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../hidet-script/3-kernel-functions.html">Kernel Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../hidet-script/4-naive-matmul.html">Naive Matrix Multiplication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../hidet-script/5-efficient-matmul.html">More Efficient Matrix Multiplication</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../hidet-script/reference/index.html">Reference</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../hidet-script/reference/1-type-system.html">Type System</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../hidet-script/reference/2-expression.html">Expressions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../hidet-script/reference/3-statement.html">Statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../hidet-script/reference/4-function.html">Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../hidet-script/reference/5-module.html">Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../hidet-script/reference/6-cuda-specific.html">CUDA Specifics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../hidet-script/reference/7-cpu-specific.html">CPU Specifics</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../how-to-guides/add-new-operator/index.html">Add New Operator</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../developer-guides/add-new-operator-compute-definition.html">Define Operator Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../developer-guides/add-new-operator-rule-based.html">Using Rule-based Scheduling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../developer-guides/add-new-operator-template-based.html">Using Template-based Scheduling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../developer-guides/add-operator-resolve-rule.html">Add Operator Resolve Rule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer-guides/add-subgraph-rewrite-rule.html">Add Sub-Graph Rewrite Rule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer-guides/contributing.html">Contributing</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notes/operator-cache.html">Operator Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../how-to-guides/visualize-flow-graph.html">Visualize Flow Graph</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../python_api/index.html">Python API</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../python_api/root.html">hidet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../python_api/option.html">hidet.option</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../python_api/cuda.html">hidet.cuda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../python_api/tensor.html">hidet.Tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../python_api/data_types.html">hidet.dtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../python_api/ops/index.html">hidet.ops</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../python_api/graph/index.html">hidet.graph</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../python_api/graph/frontend/index.html">hidet.graph.frontend</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../python_api/graph/frontend/onnx.html">hidet.graph.frontend.onnx</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python_api/graph/frontend/torch.html">hidet.graph.frontend.torch</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../python_api/graph/transforms/index.html">hidet.graph.transforms</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../python_api/graph/transforms/subgraph_rewrite.html">Sub-graph Rewrite Pass</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../python_api/graph/transforms/resolve_variant.html">Resolve Operator Pass</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../python_api/runtime/index.html">hidet.runtime</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../python_api/utils/index.html">hidet.utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../python_api/testing/index.html">hidet.testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/hidet-org/hidet" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/gallery/tutorials/optimize-pytorch-model.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimize PyTorch Model</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#search-in-a-larger-search-space">Search in a larger search space</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-the-correctness">Check the correctness</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operator-configurations">Operator configurations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-cuda-graph-to-dispatch-kernels">Use CUDA Graph to dispatch kernels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-low-precision-data-type">Use low-precision data type</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#print-the-input-graph">Print the input graph</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-gallery-tutorials-optimize-pytorch-model-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="optimize-pytorch-model">
<span id="sphx-glr-gallery-tutorials-optimize-pytorch-model-py"></span><span id="id1"></span><h1>Optimize PyTorch Model<a class="headerlink" href="#optimize-pytorch-model" title="Permalink to this heading"><span>¶</span></a></h1>
<p>Hidet provides a backend to pytorch dynamo to optimize PyTorch models. To use this backend, you need to specify ‘hidet’
as the backend when calling <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a> such as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># optimize the model with hidet provided backend &#39;hidet&#39;</span>
<span class="n">model_hidet</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;hidet&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="margin admonition note">
<p class="admonition-title">Note</p>
<p>Currently, all the operators in hidet are generated by hidet itself and
there is no dependency on kernel libraries such as cuDNN or cuBLAS. In the future, we might support to lower some
operators to these libraries if they perform better.</p>
</div>
<p>Under the hood, hidet will convert the PyTorch model to hidet’s graph representation and optimize the computation graph
(such as sub-graph rewrite and fusion, constant folding, etc.). After that, each operator will be lowered to hidet’s
scheduling system to generate the final kernel.</p>
<p>Hidet provides some configurations to control the hidet backend of torch dynamo.</p>
<section id="search-in-a-larger-search-space">
<h2>Search in a larger search space<a class="headerlink" href="#search-in-a-larger-search-space" title="Permalink to this heading"><span>¶</span></a></h2>
<p>There are some operators that are compute-intensive and their scheduling is critical to the performance. We usually need
to search in a schedule space to find the best schedule for them to achieve the best performance on given input shapes.
However, searching in a larger schedule space usually takes longer time to optimize the model. By default, hidet will
use their default schedule to generate the kernel for all input shapes. To search in a larger schedule space to get
better performance, you can configure the search space via <a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.search_space" title="hidet.graph.frontend.torch.DynamoConfig.search_space"><code class="xref py py-func docutils literal notranslate"><span class="pre">search_space()</span></code></a>
:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># There are three search spaces:</span>
<span class="c1"># 0 - use default schedule, no search [Default]</span>
<span class="c1"># 1 - search in a small schedule space (usually 1~30 schedules)</span>
<span class="c1"># 2 - search in a large schedule space (usually more than 30 schedules)</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">search_space</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># After configure the search space, you can optimize the model</span>
<a href="https://pytorch.org/docs/stable/_dynamo.html#torch._dynamo.OptimizedModule" title="torch._dynamo.OptimizedModule" class="sphx-glr-backref-module-torch-_dynamo sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_opt</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;hidet&#39;</span><span class="p">)</span>

<span class="c1"># The actual searching happens when you first run the model to know the input shapes</span>
<span class="n">outputs</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/_dynamo.html#torch._dynamo.OptimizedModule" title="torch._dynamo.OptimizedModule" class="sphx-glr-backref-module-torch-_dynamo sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_opt</span></a><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<p>Please note that the search space we set through <code class="xref py py-func docutils literal notranslate"><span class="pre">set_search_space()</span></code> will be read and
used when we first run the model, instead of when we call <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>.</p>
</section>
<section id="check-the-correctness">
<h2>Check the correctness<a class="headerlink" href="#check-the-correctness" title="Permalink to this heading"><span>¶</span></a></h2>
<p>It is important to make sure the optimized model is correct. Hidet provides a configuration to print the numerical
difference between the hidet generated operator and the original pytorch operator. You can configure it via
<a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.correctness_report" title="hidet.graph.frontend.torch.DynamoConfig.correctness_report"><code class="xref py py-func docutils literal notranslate"><span class="pre">correctness_report()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># enable the correctness checking</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">correctness_report</span><span class="p">()</span>
</pre></div>
</div>
<p>After enabling the correctness report, every time a new graph is received to compile, hidet will print the numerical
difference using the dummy inputs (for now, torch dynamo does not expose the actual inputs to backends, thus we can
not use the actual inputs). Let’s take the resnet18 model as an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.backends.cudnn</span>
<span class="kn">import</span> <span class="nn">hidet</span>

<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">x</span></a> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;pytorch/vision:v0.9.0&#39;</span><span class="p">,</span> <span class="s1">&#39;resnet18&#39;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cuda" title="torch.nn.Module.cuda" class="sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"><span class="n">model</span><span class="o">.</span><span class="n">cuda</span></a><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="k">with</span> <a href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
    <span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">correctness_report</span><span class="p">()</span>
    <a href="https://pytorch.org/docs/stable/_dynamo.html#torch._dynamo.OptimizedModule" title="torch._dynamo.OptimizedModule" class="sphx-glr-backref-module-torch-_dynamo sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_opt</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;hidet&#39;</span><span class="p">)</span>
    <a href="https://pytorch.org/docs/stable/_dynamo.html#torch._dynamo.OptimizedModule" title="torch._dynamo.OptimizedModule" class="sphx-glr-backref-module-torch-_dynamo sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_opt</span></a><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">x</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>    kind           operator                                                                          dtype    error    attention
--  -------------  --------------------------------------------------------------------------------  -------  -------  -----------
0   placeholder                                                                                      float32  0.0e+00
1   call_module    Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)      float32  0.0e+00
2   call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  1.6e-07
3   call_module    ReLU(inplace=True)                                                                float32  1.2e-07
4   call_module    MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)        float32  1.2e-07
5   call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  9.2e-07
6   call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  5.3e-07
7   call_module    ReLU(inplace=True)                                                                float32  4.8e-07
8   call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  4.3e-07
9   call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  1.0e-06
10  call_function  operator.iadd                                                                     float32  1.0e-06
11  call_module    ReLU(inplace=True)                                                                float32  1.0e-06
12  call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  1.2e-06
13  call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  8.1e-07
14  call_module    ReLU(inplace=True)                                                                float32  7.0e-07
15  call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  4.9e-07
16  call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  1.4e-06
17  call_function  operator.iadd                                                                     float32  1.5e-06
18  call_module    ReLU(inplace=True)                                                                float32  1.5e-06
19  call_module    Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)    float32  1.6e-06
20  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  8.1e-07
21  call_module    ReLU(inplace=True)                                                                float32  7.0e-07
22  call_module    Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  6.3e-07
23  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  1.1e-06
24  call_module    Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)                    float32  6.0e-07
25  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  8.0e-07
26  call_function  operator.iadd                                                                     float32  1.2e-06
27  call_module    ReLU(inplace=True)                                                                float32  1.2e-06
28  call_module    Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  1.1e-06
29  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  1.1e-06
30  call_module    ReLU(inplace=True)                                                                float32  1.1e-06
31  call_module    Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  5.7e-07
32  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  1.3e-06
33  call_function  operator.iadd                                                                     float32  1.6e-06
34  call_module    ReLU(inplace=True)                                                                float32  1.4e-06
35  call_module    Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)   float32  1.1e-06
36  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  1.1e-06
37  call_module    ReLU(inplace=True)                                                                float32  8.3e-07
38  call_module    Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  6.0e-06
39  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  5.0e-06
40  call_module    Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)                   float32  3.8e-07
41  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  3.9e-07
42  call_function  operator.iadd                                                                     float32  4.2e-06
43  call_module    ReLU(inplace=True)                                                                float32  4.0e-06
44  call_module    Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  7.0e-06
45  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  5.0e-06
46  call_module    ReLU(inplace=True)                                                                float32  4.9e-06
47  call_module    Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  2.3e-06
48  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  4.7e-06
49  call_function  operator.iadd                                                                     float32  4.7e-06
50  call_module    ReLU(inplace=True)                                                                float32  4.7e-06
51  call_module    Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)   float32  2.0e-06
52  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  1.4e-06
53  call_module    ReLU(inplace=True)                                                                float32  1.1e-06
54  call_module    Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  8.6e-07
55  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  2.6e-06
56  call_module    Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)                   float32  1.6e-06
57  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  3.2e-06
58  call_function  operator.iadd                                                                     float32  3.3e-06
59  call_module    ReLU(inplace=True)                                                                float32  2.5e-06
60  call_module    Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  2.0e-06
61  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  2.2e-06
62  call_module    ReLU(inplace=True)                                                                float32  2.0e-06
63  call_module    Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  8.7e-07
64  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  1.1e-05
65  call_function  operator.iadd                                                                     float32  1.1e-05
66  call_module    ReLU(inplace=True)                                                                float32  1.0e-05
67  call_module    AdaptiveAvgPool2d(output_size=(1, 1))                                             float32  1.5e-06
68  call_function  torch.flatten                                                                     float32  1.5e-06
69  call_module    Linear(in_features=512, out_features=1000, bias=True)                             float32  3.0e-06
70  output                                                                                           float32  3.0e-06
</pre></div>
</div>
<div class="margin admonition tip">
<p class="admonition-title">Tip</p>
<p>Usually, we can expect:</p>
<ul class="simple">
<li><p>for float32: <span class="math notranslate nohighlight">\(e_h \leq 10^{-5}\)</span>, and</p></li>
<li><p>for float16: <span class="math notranslate nohighlight">\(e_h \leq 10^{-2}\)</span>.</p></li>
</ul>
</div>
<p>The correctness report will print the harmonic mean of the absolute error and relative error for each operator:</p>
<div class="math notranslate nohighlight">
\[e_h = \frac{|actual - expected|}{|expected| + 1} \quad (\frac{1}{e_h} = \frac{1}{e_a} + \frac{1}{e_r})\]</div>
<p>where <span class="math notranslate nohighlight">\(actual\)</span>, <span class="math notranslate nohighlight">\(expected\)</span> are the actual and expected results of the operator, respectively.
The <span class="math notranslate nohighlight">\(e_a\)</span> and <span class="math notranslate nohighlight">\(e_r\)</span> are the absolute error and relative error, respectively. The harmonic mean error is
printed for each operator.</p>
</section>
<section id="operator-configurations">
<h2>Operator configurations<a class="headerlink" href="#operator-configurations" title="Permalink to this heading"><span>¶</span></a></h2>
<section id="use-cuda-graph-to-dispatch-kernels">
<h3>Use CUDA Graph to dispatch kernels<a class="headerlink" href="#use-cuda-graph-to-dispatch-kernels" title="Permalink to this heading"><span>¶</span></a></h3>
<p>Hidet provides a configuration to use CUDA Graph to dispatch kernels. CUDA Graph is a new feature in CUDA 11.0
that allows us to record the kernel dispatches and replay them later. This feature is useful when we want to
dispatch the same kernels multiple times. Hidet will enable CUDA Graph by default. You can disable it via
<a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.use_cuda_graph" title="hidet.graph.frontend.torch.DynamoConfig.use_cuda_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">use_cuda_graph()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># disable CUDA Graph</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">use_cuda_graph</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>in case you want to use PyTorch’s CUDA Graph feature.</p>
</section>
<section id="use-low-precision-data-type">
<h3>Use low-precision data type<a class="headerlink" href="#use-low-precision-data-type" title="Permalink to this heading"><span>¶</span></a></h3>
<p>Hidet provides a configuration to use low-precision data type. By default, hidet will use the same data type as
the original PyTorch model. You can configure it via <a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.use_fp16" title="hidet.graph.frontend.torch.DynamoConfig.use_fp16"><code class="xref py py-func docutils literal notranslate"><span class="pre">use_fp16()</span></code></a> and
<a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.use_fp16_reduction" title="hidet.graph.frontend.torch.DynamoConfig.use_fp16_reduction"><code class="xref py py-func docutils literal notranslate"><span class="pre">use_fp16_reduction()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># automatically transform the model to use float16 data type</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">use_fp16</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># use float16 data type as the accumulate data type in operators with reduction</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">use_fp16_reduction</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>You do not need to change the inputs feed to the model, as hidet will automatically cast the inputs to the
configured data type automatically in the optimized model.</p>
</section>
<section id="print-the-input-graph">
<h3>Print the input graph<a class="headerlink" href="#print-the-input-graph" title="Permalink to this heading"><span>¶</span></a></h3>
<p>If you are interested in the graph that PyTorch dynamo dispatches to hidet backend, you can configure hidet to
print the graph via <a class="reference internal" href="../../python_api/graph/frontend/torch.html#hidet.graph.frontend.torch.DynamoConfig.print_input_graph" title="hidet.graph.frontend.torch.DynamoConfig.print_input_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">print_input_graph()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># print the input graph</span>
<span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">print_input_graph</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Because ResNet18 is a neat model without control flow, we can print the input graph to see how PyTorch dynamo
dispatches the model to hidet backend:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <a href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad" title="torch.no_grad" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"><span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span></a><span class="p">():</span>
    <span class="n">hidet</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">dynamo_config</span><span class="o">.</span><span class="n">print_input_graph</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <a href="https://pytorch.org/docs/stable/_dynamo.html#torch._dynamo.OptimizedModule" title="torch._dynamo.OptimizedModule" class="sphx-glr-backref-module-torch-_dynamo sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_opt</span></a> <span class="o">=</span> <a href="https://pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="torch.compile" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"><span class="n">torch</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;hidet&#39;</span><span class="p">)</span>
    <a href="https://pytorch.org/docs/stable/_dynamo.html#torch._dynamo.OptimizedModule" title="torch._dynamo.OptimizedModule" class="sphx-glr-backref-module-torch-_dynamo sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_opt</span></a><span class="p">(</span><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" class="sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">x</span></a><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>class GraphModule(torch.nn.Module):
    def forward(self, x : torch.Tensor):
        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:232, code: x = self.conv1(x)
        self_conv1 = self.self_conv1(x);  x = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:233, code: x = self.bn1(x)
        self_bn1 = self.self_bn1(self_conv1);  self_conv1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:234, code: x = self.relu(x)
        self_relu = self.self_relu(self_bn1);  self_bn1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:235, code: x = self.maxpool(x)
        self_maxpool = self.self_maxpool(self_relu);  self_relu = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:70, code: out = self.conv1(x)
        self_layer1_0_conv1 = self.self_layer1_0_conv1(self_maxpool)

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:71, code: out = self.bn1(out)
        self_layer1_0_bn1 = self.self_layer1_0_bn1(self_layer1_0_conv1);  self_layer1_0_conv1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:72, code: out = self.relu(out)
        self_layer1_0_relu = self.self_layer1_0_relu(self_layer1_0_bn1);  self_layer1_0_bn1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:74, code: out = self.conv2(out)
        self_layer1_0_conv2 = self.self_layer1_0_conv2(self_layer1_0_relu);  self_layer1_0_relu = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:75, code: out = self.bn2(out)
        self_layer1_0_bn2 = self.self_layer1_0_bn2(self_layer1_0_conv2);  self_layer1_0_conv2 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:80, code: out += identity
        self_layer1_0_bn2 += self_maxpool;  iadd = self_layer1_0_bn2;  self_layer1_0_bn2 = self_maxpool = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:81, code: out = self.relu(out)
        self_layer1_0_relu_1 = self.self_layer1_0_relu(iadd);  iadd = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:70, code: out = self.conv1(x)
        self_layer1_1_conv1 = self.self_layer1_1_conv1(self_layer1_0_relu_1)

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:71, code: out = self.bn1(out)
        self_layer1_1_bn1 = self.self_layer1_1_bn1(self_layer1_1_conv1);  self_layer1_1_conv1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:72, code: out = self.relu(out)
        self_layer1_1_relu = self.self_layer1_1_relu(self_layer1_1_bn1);  self_layer1_1_bn1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:74, code: out = self.conv2(out)
        self_layer1_1_conv2 = self.self_layer1_1_conv2(self_layer1_1_relu);  self_layer1_1_relu = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:75, code: out = self.bn2(out)
        self_layer1_1_bn2 = self.self_layer1_1_bn2(self_layer1_1_conv2);  self_layer1_1_conv2 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:80, code: out += identity
        self_layer1_1_bn2 += self_layer1_0_relu_1;  iadd_1 = self_layer1_1_bn2;  self_layer1_1_bn2 = self_layer1_0_relu_1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:81, code: out = self.relu(out)
        self_layer1_1_relu_1 = self.self_layer1_1_relu(iadd_1);  iadd_1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:70, code: out = self.conv1(x)
        self_layer2_0_conv1 = self.self_layer2_0_conv1(self_layer1_1_relu_1)

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:71, code: out = self.bn1(out)
        self_layer2_0_bn1 = self.self_layer2_0_bn1(self_layer2_0_conv1);  self_layer2_0_conv1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:72, code: out = self.relu(out)
        self_layer2_0_relu = self.self_layer2_0_relu(self_layer2_0_bn1);  self_layer2_0_bn1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:74, code: out = self.conv2(out)
        self_layer2_0_conv2 = self.self_layer2_0_conv2(self_layer2_0_relu);  self_layer2_0_relu = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:75, code: out = self.bn2(out)
        self_layer2_0_bn2 = self.self_layer2_0_bn2(self_layer2_0_conv2);  self_layer2_0_conv2 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:78, code: identity = self.downsample(x)
        self_layer2_0_downsample_0 = self.self_layer2_0_downsample_0(self_layer1_1_relu_1);  self_layer1_1_relu_1 = None
        self_layer2_0_downsample_1 = self.self_layer2_0_downsample_1(self_layer2_0_downsample_0);  self_layer2_0_downsample_0 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:80, code: out += identity
        self_layer2_0_bn2 += self_layer2_0_downsample_1;  iadd_2 = self_layer2_0_bn2;  self_layer2_0_bn2 = self_layer2_0_downsample_1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:81, code: out = self.relu(out)
        self_layer2_0_relu_1 = self.self_layer2_0_relu(iadd_2);  iadd_2 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:70, code: out = self.conv1(x)
        self_layer2_1_conv1 = self.self_layer2_1_conv1(self_layer2_0_relu_1)

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:71, code: out = self.bn1(out)
        self_layer2_1_bn1 = self.self_layer2_1_bn1(self_layer2_1_conv1);  self_layer2_1_conv1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:72, code: out = self.relu(out)
        self_layer2_1_relu = self.self_layer2_1_relu(self_layer2_1_bn1);  self_layer2_1_bn1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:74, code: out = self.conv2(out)
        self_layer2_1_conv2 = self.self_layer2_1_conv2(self_layer2_1_relu);  self_layer2_1_relu = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:75, code: out = self.bn2(out)
        self_layer2_1_bn2 = self.self_layer2_1_bn2(self_layer2_1_conv2);  self_layer2_1_conv2 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:80, code: out += identity
        self_layer2_1_bn2 += self_layer2_0_relu_1;  iadd_3 = self_layer2_1_bn2;  self_layer2_1_bn2 = self_layer2_0_relu_1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:81, code: out = self.relu(out)
        self_layer2_1_relu_1 = self.self_layer2_1_relu(iadd_3);  iadd_3 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:70, code: out = self.conv1(x)
        self_layer3_0_conv1 = self.self_layer3_0_conv1(self_layer2_1_relu_1)

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:71, code: out = self.bn1(out)
        self_layer3_0_bn1 = self.self_layer3_0_bn1(self_layer3_0_conv1);  self_layer3_0_conv1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:72, code: out = self.relu(out)
        self_layer3_0_relu = self.self_layer3_0_relu(self_layer3_0_bn1);  self_layer3_0_bn1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:74, code: out = self.conv2(out)
        self_layer3_0_conv2 = self.self_layer3_0_conv2(self_layer3_0_relu);  self_layer3_0_relu = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:75, code: out = self.bn2(out)
        self_layer3_0_bn2 = self.self_layer3_0_bn2(self_layer3_0_conv2);  self_layer3_0_conv2 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:78, code: identity = self.downsample(x)
        self_layer3_0_downsample_0 = self.self_layer3_0_downsample_0(self_layer2_1_relu_1);  self_layer2_1_relu_1 = None
        self_layer3_0_downsample_1 = self.self_layer3_0_downsample_1(self_layer3_0_downsample_0);  self_layer3_0_downsample_0 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:80, code: out += identity
        self_layer3_0_bn2 += self_layer3_0_downsample_1;  iadd_4 = self_layer3_0_bn2;  self_layer3_0_bn2 = self_layer3_0_downsample_1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:81, code: out = self.relu(out)
        self_layer3_0_relu_1 = self.self_layer3_0_relu(iadd_4);  iadd_4 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:70, code: out = self.conv1(x)
        self_layer3_1_conv1 = self.self_layer3_1_conv1(self_layer3_0_relu_1)

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:71, code: out = self.bn1(out)
        self_layer3_1_bn1 = self.self_layer3_1_bn1(self_layer3_1_conv1);  self_layer3_1_conv1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:72, code: out = self.relu(out)
        self_layer3_1_relu = self.self_layer3_1_relu(self_layer3_1_bn1);  self_layer3_1_bn1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:74, code: out = self.conv2(out)
        self_layer3_1_conv2 = self.self_layer3_1_conv2(self_layer3_1_relu);  self_layer3_1_relu = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:75, code: out = self.bn2(out)
        self_layer3_1_bn2 = self.self_layer3_1_bn2(self_layer3_1_conv2);  self_layer3_1_conv2 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:80, code: out += identity
        self_layer3_1_bn2 += self_layer3_0_relu_1;  iadd_5 = self_layer3_1_bn2;  self_layer3_1_bn2 = self_layer3_0_relu_1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:81, code: out = self.relu(out)
        self_layer3_1_relu_1 = self.self_layer3_1_relu(iadd_5);  iadd_5 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:70, code: out = self.conv1(x)
        self_layer4_0_conv1 = self.self_layer4_0_conv1(self_layer3_1_relu_1)

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:71, code: out = self.bn1(out)
        self_layer4_0_bn1 = self.self_layer4_0_bn1(self_layer4_0_conv1);  self_layer4_0_conv1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:72, code: out = self.relu(out)
        self_layer4_0_relu = self.self_layer4_0_relu(self_layer4_0_bn1);  self_layer4_0_bn1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:74, code: out = self.conv2(out)
        self_layer4_0_conv2 = self.self_layer4_0_conv2(self_layer4_0_relu);  self_layer4_0_relu = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:75, code: out = self.bn2(out)
        self_layer4_0_bn2 = self.self_layer4_0_bn2(self_layer4_0_conv2);  self_layer4_0_conv2 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:78, code: identity = self.downsample(x)
        self_layer4_0_downsample_0 = self.self_layer4_0_downsample_0(self_layer3_1_relu_1);  self_layer3_1_relu_1 = None
        self_layer4_0_downsample_1 = self.self_layer4_0_downsample_1(self_layer4_0_downsample_0);  self_layer4_0_downsample_0 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:80, code: out += identity
        self_layer4_0_bn2 += self_layer4_0_downsample_1;  iadd_6 = self_layer4_0_bn2;  self_layer4_0_bn2 = self_layer4_0_downsample_1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:81, code: out = self.relu(out)
        self_layer4_0_relu_1 = self.self_layer4_0_relu(iadd_6);  iadd_6 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:70, code: out = self.conv1(x)
        self_layer4_1_conv1 = self.self_layer4_1_conv1(self_layer4_0_relu_1)

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:71, code: out = self.bn1(out)
        self_layer4_1_bn1 = self.self_layer4_1_bn1(self_layer4_1_conv1);  self_layer4_1_conv1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:72, code: out = self.relu(out)
        self_layer4_1_relu = self.self_layer4_1_relu(self_layer4_1_bn1);  self_layer4_1_bn1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:74, code: out = self.conv2(out)
        self_layer4_1_conv2 = self.self_layer4_1_conv2(self_layer4_1_relu);  self_layer4_1_relu = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:75, code: out = self.bn2(out)
        self_layer4_1_bn2 = self.self_layer4_1_bn2(self_layer4_1_conv2);  self_layer4_1_conv2 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:80, code: out += identity
        self_layer4_1_bn2 += self_layer4_0_relu_1;  iadd_7 = self_layer4_1_bn2;  self_layer4_1_bn2 = self_layer4_0_relu_1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:81, code: out = self.relu(out)
        self_layer4_1_relu_1 = self.self_layer4_1_relu(iadd_7);  iadd_7 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:242, code: x = self.avgpool(x)
        self_avgpool = self.self_avgpool(self_layer4_1_relu_1);  self_layer4_1_relu_1 = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:243, code: x = torch.flatten(x, 1)
        flatten = torch.flatten(self_avgpool, 1);  self_avgpool = None

        # File: /home/yaoyao/.cache/torch/hub/pytorch_vision_v0.9.0/torchvision/models/resnet.py:244, code: x = self.fc(x)
        self_fc = self.self_fc(flatten);  flatten = None
        return (self_fc,)

---
opcode         name                        target                                                      args                                             kwargs
-------------  --------------------------  ----------------------------------------------------------  -----------------------------------------------  --------
placeholder    x                           x                                                           ()                                               {}
call_module    self_conv1                  self_conv1                                                  (x,)                                             {}
call_module    self_bn1                    self_bn1                                                    (self_conv1,)                                    {}
call_module    self_relu                   self_relu                                                   (self_bn1,)                                      {}
call_module    self_maxpool                self_maxpool                                                (self_relu,)                                     {}
call_module    self_layer1_0_conv1         self_layer1_0_conv1                                         (self_maxpool,)                                  {}
call_module    self_layer1_0_bn1           self_layer1_0_bn1                                           (self_layer1_0_conv1,)                           {}
call_module    self_layer1_0_relu          self_layer1_0_relu                                          (self_layer1_0_bn1,)                             {}
call_module    self_layer1_0_conv2         self_layer1_0_conv2                                         (self_layer1_0_relu,)                            {}
call_module    self_layer1_0_bn2           self_layer1_0_bn2                                           (self_layer1_0_conv2,)                           {}
call_function  iadd                        &lt;built-in function iadd&gt;                                    (self_layer1_0_bn2, self_maxpool)                {}
call_module    self_layer1_0_relu_1        self_layer1_0_relu                                          (iadd,)                                          {}
call_module    self_layer1_1_conv1         self_layer1_1_conv1                                         (self_layer1_0_relu_1,)                          {}
call_module    self_layer1_1_bn1           self_layer1_1_bn1                                           (self_layer1_1_conv1,)                           {}
call_module    self_layer1_1_relu          self_layer1_1_relu                                          (self_layer1_1_bn1,)                             {}
call_module    self_layer1_1_conv2         self_layer1_1_conv2                                         (self_layer1_1_relu,)                            {}
call_module    self_layer1_1_bn2           self_layer1_1_bn2                                           (self_layer1_1_conv2,)                           {}
call_function  iadd_1                      &lt;built-in function iadd&gt;                                    (self_layer1_1_bn2, self_layer1_0_relu_1)        {}
call_module    self_layer1_1_relu_1        self_layer1_1_relu                                          (iadd_1,)                                        {}
call_module    self_layer2_0_conv1         self_layer2_0_conv1                                         (self_layer1_1_relu_1,)                          {}
call_module    self_layer2_0_bn1           self_layer2_0_bn1                                           (self_layer2_0_conv1,)                           {}
call_module    self_layer2_0_relu          self_layer2_0_relu                                          (self_layer2_0_bn1,)                             {}
call_module    self_layer2_0_conv2         self_layer2_0_conv2                                         (self_layer2_0_relu,)                            {}
call_module    self_layer2_0_bn2           self_layer2_0_bn2                                           (self_layer2_0_conv2,)                           {}
call_module    self_layer2_0_downsample_0  self_layer2_0_downsample_0                                  (self_layer1_1_relu_1,)                          {}
call_module    self_layer2_0_downsample_1  self_layer2_0_downsample_1                                  (self_layer2_0_downsample_0,)                    {}
call_function  iadd_2                      &lt;built-in function iadd&gt;                                    (self_layer2_0_bn2, self_layer2_0_downsample_1)  {}
call_module    self_layer2_0_relu_1        self_layer2_0_relu                                          (iadd_2,)                                        {}
call_module    self_layer2_1_conv1         self_layer2_1_conv1                                         (self_layer2_0_relu_1,)                          {}
call_module    self_layer2_1_bn1           self_layer2_1_bn1                                           (self_layer2_1_conv1,)                           {}
call_module    self_layer2_1_relu          self_layer2_1_relu                                          (self_layer2_1_bn1,)                             {}
call_module    self_layer2_1_conv2         self_layer2_1_conv2                                         (self_layer2_1_relu,)                            {}
call_module    self_layer2_1_bn2           self_layer2_1_bn2                                           (self_layer2_1_conv2,)                           {}
call_function  iadd_3                      &lt;built-in function iadd&gt;                                    (self_layer2_1_bn2, self_layer2_0_relu_1)        {}
call_module    self_layer2_1_relu_1        self_layer2_1_relu                                          (iadd_3,)                                        {}
call_module    self_layer3_0_conv1         self_layer3_0_conv1                                         (self_layer2_1_relu_1,)                          {}
call_module    self_layer3_0_bn1           self_layer3_0_bn1                                           (self_layer3_0_conv1,)                           {}
call_module    self_layer3_0_relu          self_layer3_0_relu                                          (self_layer3_0_bn1,)                             {}
call_module    self_layer3_0_conv2         self_layer3_0_conv2                                         (self_layer3_0_relu,)                            {}
call_module    self_layer3_0_bn2           self_layer3_0_bn2                                           (self_layer3_0_conv2,)                           {}
call_module    self_layer3_0_downsample_0  self_layer3_0_downsample_0                                  (self_layer2_1_relu_1,)                          {}
call_module    self_layer3_0_downsample_1  self_layer3_0_downsample_1                                  (self_layer3_0_downsample_0,)                    {}
call_function  iadd_4                      &lt;built-in function iadd&gt;                                    (self_layer3_0_bn2, self_layer3_0_downsample_1)  {}
call_module    self_layer3_0_relu_1        self_layer3_0_relu                                          (iadd_4,)                                        {}
call_module    self_layer3_1_conv1         self_layer3_1_conv1                                         (self_layer3_0_relu_1,)                          {}
call_module    self_layer3_1_bn1           self_layer3_1_bn1                                           (self_layer3_1_conv1,)                           {}
call_module    self_layer3_1_relu          self_layer3_1_relu                                          (self_layer3_1_bn1,)                             {}
call_module    self_layer3_1_conv2         self_layer3_1_conv2                                         (self_layer3_1_relu,)                            {}
call_module    self_layer3_1_bn2           self_layer3_1_bn2                                           (self_layer3_1_conv2,)                           {}
call_function  iadd_5                      &lt;built-in function iadd&gt;                                    (self_layer3_1_bn2, self_layer3_0_relu_1)        {}
call_module    self_layer3_1_relu_1        self_layer3_1_relu                                          (iadd_5,)                                        {}
call_module    self_layer4_0_conv1         self_layer4_0_conv1                                         (self_layer3_1_relu_1,)                          {}
call_module    self_layer4_0_bn1           self_layer4_0_bn1                                           (self_layer4_0_conv1,)                           {}
call_module    self_layer4_0_relu          self_layer4_0_relu                                          (self_layer4_0_bn1,)                             {}
call_module    self_layer4_0_conv2         self_layer4_0_conv2                                         (self_layer4_0_relu,)                            {}
call_module    self_layer4_0_bn2           self_layer4_0_bn2                                           (self_layer4_0_conv2,)                           {}
call_module    self_layer4_0_downsample_0  self_layer4_0_downsample_0                                  (self_layer3_1_relu_1,)                          {}
call_module    self_layer4_0_downsample_1  self_layer4_0_downsample_1                                  (self_layer4_0_downsample_0,)                    {}
call_function  iadd_6                      &lt;built-in function iadd&gt;                                    (self_layer4_0_bn2, self_layer4_0_downsample_1)  {}
call_module    self_layer4_0_relu_1        self_layer4_0_relu                                          (iadd_6,)                                        {}
call_module    self_layer4_1_conv1         self_layer4_1_conv1                                         (self_layer4_0_relu_1,)                          {}
call_module    self_layer4_1_bn1           self_layer4_1_bn1                                           (self_layer4_1_conv1,)                           {}
call_module    self_layer4_1_relu          self_layer4_1_relu                                          (self_layer4_1_bn1,)                             {}
call_module    self_layer4_1_conv2         self_layer4_1_conv2                                         (self_layer4_1_relu,)                            {}
call_module    self_layer4_1_bn2           self_layer4_1_bn2                                           (self_layer4_1_conv2,)                           {}
call_function  iadd_7                      &lt;built-in function iadd&gt;                                    (self_layer4_1_bn2, self_layer4_0_relu_1)        {}
call_module    self_layer4_1_relu_1        self_layer4_1_relu                                          (iadd_7,)                                        {}
call_module    self_avgpool                self_avgpool                                                (self_layer4_1_relu_1,)                          {}
call_function  flatten                     &lt;built-in method flatten of type object at 0x7f9b0c75b500&gt;  (self_avgpool, 1)                                {}
call_module    self_fc                     self_fc                                                     (flatten,)                                       {}
output         output                      output                                                      ((self_fc,),)                                    {}
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 2.906 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-gallery-tutorials-optimize-pytorch-model-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/a824231294cbc050bb47a52e6857032d/optimize-pytorch-model.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">optimize-pytorch-model.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/e571565f59b2a0466491712c8f894b70/optimize-pytorch-model.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">optimize-pytorch-model.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../getting-started/quick-start.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Quick Start</p>
      </div>
    </a>
    <a class="right-next"
       href="optimize-onnx-model.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Optimize ONNX Model</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#search-in-a-larger-search-space">Search in a larger search space</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#check-the-correctness">Check the correctness</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#operator-configurations">Operator configurations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-cuda-graph-to-dispatch-kernels">Use CUDA Graph to dispatch kernels</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-low-precision-data-type">Use low-precision data type</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#print-the-input-graph">Print the input graph</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Hidet Team
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023, Hidet Authors.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>