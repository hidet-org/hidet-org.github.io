{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Optimize PyTorch Model\n\nHidet provides a backend to pytorch dynamo to optimize PyTorch models. To use this backend, you need to specify 'hidet'\nas the backend when calling :func:`torch.compile` such as\n\n```python\n# optimize the model with hidet provided backend 'hidet'\nmodel_hidet = torch.compile(model, backend='hidet')\n```\n<div class=\"alert alert-info\"><h4>Note</h4><p>:class: margin\n\n  Currently, all the operators in hidet are generated by hidet itself and\n  there is no dependency on kernel libraries such as cuDNN or cuBLAS. In the future, we might support to lower some\n  operators to these libraries if they perform better.</p></div>\n\nUnder the hood, hidet will convert the PyTorch model to hidet's graph representation and optimize the computation graph\n(such as sub-graph rewrite and fusion, constant folding, etc.). After that, each operator will be lowered to hidet's\nscheduling system to generate the final kernel.\n\n\nHidet provides some configurations to control the hidet backend of torch dynamo.\n\n## Search in a larger search space\nThere are some operators that are compute-intensive and their scheduling is critical to the performance. We usually need\nto search in a schedule space to find the best schedule for them to achieve the best performance on given input shapes.\nHowever, searching in a larger schedule space usually takes longer time to optimize the model. By default, hidet will\nuse their default schedule to generate the kernel for all input shapes. To search in a larger schedule space to get\nbetter performance, you can configure the search space via :func:`~hidet.graph.frontend.torch.DynamoConfig.search_space`\n:\n\n```python\n# There are three search spaces:\n# 0 - use default schedule, no search [Default]\n# 1 - search in a small schedule space (usually 1~30 schedules)\n# 2 - search in a large schedule space (usually more than 30 schedules)\nhidet.torch.dynamo_config.search_space(2)\n\n# After configure the search space, you can optimize the model\nmodel_opt = torch.compile(model, backend='hidet')\n\n# The actual searching happens when you first run the model to know the input shapes\noutputs = model_opt(inputs)\n```\nPlease note that the search space we set through :func:`~hidet.torch.dynamo_config.set_search_space` will be read and\nused when we first run the model, instead of when we call :func:`torch.compile`.\n\n## Check the correctness\nIt is important to make sure the optimized model is correct. Hidet provides a configuration to print the numerical\ndifference between the hidet generated operator and the original pytorch operator. You can configure it via\n:func:`~hidet.graph.frontend.torch.DynamoConfig.correctness_report`:\n\n```python\n# enable the correctness checking\nhidet.torch.dynamo_config.correctness_report()\n```\nAfter enabling the correctness report, every time a new graph is received to compile, hidet will print the numerical\ndifference using the dummy inputs (for now, torch dynamo does not expose the actual inputs to backends, thus we can\nnot use the actual inputs). Let's take the resnet18 model as an example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.backends.cudnn\nimport hidet\n\nx = torch.randn(1, 3, 224, 224).cuda()\nmodel = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=True, verbose=False)\nmodel = model.cuda().eval()\n\nwith torch.no_grad():\n    hidet.torch.dynamo_config.correctness_report()\n    model_opt = torch.compile(model, backend='hidet')\n    model_opt(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. tip::\n  :class: margin\n\n  Usually, we can expect:\n\n  - for float32: $e_h \\leq 10^{-5}$, and\n  - for float16: $e_h \\leq 10^{-2}$.\n\nThe correctness report will print the harmonic mean of the absolute error and relative error for each operator:\n\n\\begin{align}e_h = \\frac{|actual - expected|}{|expected| + 1} \\quad (\\frac{1}{e_h} = \\frac{1}{e_a} + \\frac{1}{e_r})\\end{align}\n\n\nwhere $actual$, $expected$ are the actual and expected results of the operator, respectively.\nThe $e_a$ and $e_r$ are the absolute error and relative error, respectively. The harmonic mean error is\nprinted for each operator.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Operator configurations\n\n### Use CUDA Graph to dispatch kernels\n\nHidet provides a configuration to use CUDA Graph to dispatch kernels. CUDA Graph is a new feature in CUDA 11.0\nthat allows us to record the kernel dispatches and replay them later. This feature is useful when we want to\ndispatch the same kernels multiple times. Hidet will enable CUDA Graph by default. You can disable it via\n:func:`~hidet.graph.frontend.torch.DynamoConfig.use_cuda_graph`:\n\n```python\n# disable CUDA Graph\nhidet.torch.dynamo_config.use_cuda_graph(False)\n```\nin case you want to use PyTorch's CUDA Graph feature.\n\n### Use low-precision data type\n\nHidet provides a configuration to use low-precision data type. By default, hidet will use the same data type as\nthe original PyTorch model. You can configure it via :func:`~hidet.graph.frontend.torch.DynamoConfig.use_fp16` and\n:func:`~hidet.graph.frontend.torch.DynamoConfig.use_fp16_reduction`:\n\n```python\n# automatically transform the model to use float16 data type\nhidet.torch.dynamo_config.use_fp16(True)\n\n# use float16 data type as the accumulate data type in operators with reduction\nhidet.torch.dynamo_config.use_fp16_reduction(True)\n```\nYou do not need to change the inputs feed to the model, as hidet will automatically cast the inputs to the\nconfigured data type automatically in the optimized model.\n\n\n### Print the input graph\n\nIf you are interested in the graph that PyTorch dynamo dispatches to hidet backend, you can configure hidet to\nprint the graph via :func:`~hidet.graph.frontend.torch.DynamoConfig.print_input_graph`:\n\n```python\n# print the input graph\nhidet.torch.dynamo_config.print_input_graph(True)\n```\nBecause ResNet18 is a neat model without control flow, we can print the input graph to see how PyTorch dynamo\ndispatches the model to hidet backend:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n    hidet.torch.dynamo_config.print_input_graph(True)\n    model_opt = torch.compile(model, backend='hidet')\n    model_opt(x)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}