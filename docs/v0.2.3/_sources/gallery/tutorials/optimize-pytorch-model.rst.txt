
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "gallery/tutorials/optimize-pytorch-model.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_gallery_tutorials_optimize-pytorch-model.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_gallery_tutorials_optimize-pytorch-model.py:


Optimize PyTorch Model
======================

Hidet provides a backend to pytorch dynamo to optimize PyTorch models. To use this backend, you need to specify 'hidet'
as the backend when calling :func:`torch.compile` such as

.. code-block:: python

    # optimize the model with hidet provided backend 'hidet'
    model_hidet = torch.compile(model, backend='hidet')

.. note::
  :class: margin

  Currently, all the operators in hidet are generated by hidet itself and
  there is no dependency on kernel libraries such as cuDNN or cuBLAS. In the future, we might support to lower some
  operators to these libraries if they perform better.

Under the hood, hidet will convert the PyTorch model to hidet's graph representation and optimize the computation graph
(such as sub-graph rewrite and fusion, constant folding, etc.). After that, each operator will be lowered to hidet's
scheduling system to generate the final kernel.


Hidet provides some configurations to control the hidet backend of torch dynamo.

Search in a larger search space
-------------------------------
There are some operators that are compute-intensive and their scheduling is critical to the performance. We usually need
to search in a schedule space to find the best schedule for them to achieve the best performance on given input shapes.
However, searching in a larger schedule space usually takes longer time to optimize the model. By default, hidet will
use their default schedule to generate the kernel for all input shapes. To search in a larger schedule space to get
better performance, you can configure the search space via :func:`~hidet.graph.frontend.torch.DynamoConfig.search_space`
:

.. code-block:: python

    # There are three search spaces:
    # 0 - use default schedule, no search [Default]
    # 1 - search in a small schedule space (usually 1~30 schedules)
    # 2 - search in a large schedule space (usually more than 30 schedules)
    hidet.torch.dynamo_config.search_space(2)

    # After configure the search space, you can optimize the model
    model_opt = torch.compile(model, backend='hidet')

    # The actual searching happens when you first run the model to know the input shapes
    outputs = model_opt(inputs)

Please note that the search space we set through :func:`~hidet.torch.dynamo_config.set_search_space` will be read and
used when we first run the model, instead of when we call :func:`torch.compile`.

Check the correctness
---------------------
It is important to make sure the optimized model is correct. Hidet provides a configuration to print the numerical
difference between the hidet generated operator and the original pytorch operator. You can configure it via
:func:`~hidet.graph.frontend.torch.DynamoConfig.correctness_report`:

.. code-block:: python

    # enable the correctness checking
    hidet.torch.dynamo_config.correctness_report()

After enabling the correctness report, every time a new graph is received to compile, hidet will print the numerical
difference using the dummy inputs (for now, torch dynamo does not expose the actual inputs to backends, thus we can
not use the actual inputs). Let's take the resnet18 model as an example:

.. GENERATED FROM PYTHON SOURCE LINES 68-82

.. code-block:: default

    import torch.backends.cudnn
    import hidet

    x = torch.randn(1, 3, 224, 224).cuda()
    model = torch.hub.load(
        'pytorch/vision:v0.9.0', 'resnet18', pretrained=True, verbose=False
    )
    model = model.cuda().eval()

    with torch.no_grad():
        hidet.torch.dynamo_config.correctness_report()
        model_opt = torch.compile(model, backend='hidet')
        model_opt(x)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

        kind           operator                                                                          dtype    error    attention
    --  -------------  --------------------------------------------------------------------------------  -------  -------  -----------
    0   placeholder                                                                                      float32  0.0e+00
    1   call_module    Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)      float32  0.0e+00
    2   call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  1.5e-07
    3   call_module    ReLU(inplace=True)                                                                float32  1.1e-07
    4   call_module    MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)        float32  1.1e-07
    5   call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  2.0e-06
    6   call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  2.0e-06
    7   call_module    ReLU(inplace=True)                                                                float32  2.0e-06
    8   call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  1.0e-06
    9   call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  1.9e-06
    10  call_function  operator.iadd                                                                     float32  2.1e-06
    11  call_module    ReLU(inplace=True)                                                                float32  1.9e-06
    12  call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  2.0e-06
    13  call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  1.7e-06
    14  call_module    ReLU(inplace=True)                                                                float32  1.7e-06
    15  call_module    Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)     float32  1.1e-06
    16  call_module    BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   float32  2.7e-06
    17  call_function  operator.iadd                                                                     float32  2.9e-06
    18  call_module    ReLU(inplace=True)                                                                float32  2.9e-06
    19  call_module    Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)    float32  1.3e-03  <------
    20  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  5.3e-04  <------
    21  call_module    ReLU(inplace=True)                                                                float32  5.3e-04  <------
    22  call_module    Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  4.5e-04  <------
    23  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  7.0e-04  <------
    24  call_module    Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)                    float32  1.2e-06
    25  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  1.6e-06
    26  call_function  operator.iadd                                                                     float32  6.8e-04  <------
    27  call_module    ReLU(inplace=True)                                                                float32  6.8e-04  <------
    28  call_module    Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  7.4e-04  <------
    29  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  5.6e-04  <------
    30  call_module    ReLU(inplace=True)                                                                float32  4.8e-04  <------
    31  call_module    Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  4.2e-04  <------
    32  call_module    BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  1.1e-03  <------
    33  call_function  operator.iadd                                                                     float32  1.3e-03  <------
    34  call_module    ReLU(inplace=True)                                                                float32  9.1e-04  <------
    35  call_module    Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)   float32  9.3e-04  <------
    36  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  8.3e-04  <------
    37  call_module    ReLU(inplace=True)                                                                float32  7.2e-04  <------
    38  call_module    Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  5.7e-04  <------
    39  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  6.4e-04  <------
    40  call_module    Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)                   float32  2.9e-04  <------
    41  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  2.9e-04  <------
    42  call_function  operator.iadd                                                                     float32  6.5e-04  <------
    43  call_module    ReLU(inplace=True)                                                                float32  6.5e-04  <------
    44  call_module    Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  6.6e-04  <------
    45  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  4.9e-04  <------
    46  call_module    ReLU(inplace=True)                                                                float32  4.9e-04  <------
    47  call_module    Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  3.6e-04  <------
    48  call_module    BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  7.7e-04  <------
    49  call_function  operator.iadd                                                                     float32  8.7e-04  <------
    50  call_module    ReLU(inplace=True)                                                                float32  7.4e-04  <------
    51  call_module    Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)   float32  6.7e-04  <------
    52  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  6.3e-04  <------
    53  call_module    ReLU(inplace=True)                                                                float32  4.2e-04  <------
    54  call_module    Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  3.1e-04  <------
    55  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  7.7e-04  <------
    56  call_module    Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)                   float32  3.0e-04  <------
    57  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  6.3e-04  <------
    58  call_function  operator.iadd                                                                     float32  8.1e-04  <------
    59  call_module    ReLU(inplace=True)                                                                float32  8.1e-04  <------
    60  call_module    Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  5.6e-04  <------
    61  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  5.9e-04  <------
    62  call_module    ReLU(inplace=True)                                                                float32  4.7e-04  <------
    63  call_module    Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)   float32  3.3e-04  <------
    64  call_module    BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  float32  3.4e-03  <------
    65  call_function  operator.iadd                                                                     float32  3.5e-03  <------
    66  call_module    ReLU(inplace=True)                                                                float32  3.5e-03  <------
    67  call_module    AdaptiveAvgPool2d(output_size=(1, 1))                                             float32  8.3e-04  <------
    68  call_function  torch.flatten                                                                     float32  8.3e-04  <------
    69  call_module    Linear(in_features=512, out_features=1000, bias=True)                             float32  1.7e-03  <------
    70  output                                                                                           float32  1.7e-03  <------




.. GENERATED FROM PYTHON SOURCE LINES 83-101

.. tip::
  :class: margin

  Usually, we can expect:

  - for float32: :math:`e_h \leq 10^{-5}`, and
  - for float16: :math:`e_h \leq 10^{-2}`.

The correctness report will print the harmonic mean of the absolute error and relative error for each operator:

.. math::
  e_h = \frac{|actual - expected|}{|expected| + 1} \quad (\frac{1}{e_h} = \frac{1}{e_a} + \frac{1}{e_r})


where :math:`actual`, :math:`expected` are the actual and expected results of the operator, respectively.
The :math:`e_a` and :math:`e_r` are the absolute error and relative error, respectively. The harmonic mean error is
printed for each operator.


.. GENERATED FROM PYTHON SOURCE LINES 104-154

Operator configurations
-----------------------

Use CUDA Graph to dispatch kernels
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Hidet provides a configuration to use CUDA Graph to dispatch kernels. CUDA Graph is a new feature in CUDA 11.0
that allows us to record the kernel dispatches and replay them later. This feature is useful when we want to
dispatch the same kernels multiple times. Hidet will enable CUDA Graph by default. You can disable it via
:func:`~hidet.graph.frontend.torch.DynamoConfig.use_cuda_graph`:

.. code-block:: python

    # disable CUDA Graph
    hidet.torch.dynamo_config.use_cuda_graph(False)

in case you want to use PyTorch's CUDA Graph feature.

Use low-precision data type
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Hidet provides a configuration to use low-precision data type. By default, hidet will use the same data type as
the original PyTorch model. You can configure it via :func:`~hidet.graph.frontend.torch.DynamoConfig.use_fp16` and
:func:`~hidet.graph.frontend.torch.DynamoConfig.use_fp16_reduction`:

.. code-block:: python

    # automatically transform the model to use float16 data type
    hidet.torch.dynamo_config.use_fp16(True)

    # use float16 data type as the accumulate data type in operators with reduction
    hidet.torch.dynamo_config.use_fp16_reduction(True)

You do not need to change the inputs feed to the model, as hidet will automatically cast the inputs to the
configured data type automatically in the optimized model.


Print the input graph
~~~~~~~~~~~~~~~~~~~~~

If you are interested in the graph that PyTorch dynamo dispatches to hidet backend, you can configure hidet to
print the graph via :func:`~hidet.graph.frontend.torch.DynamoConfig.print_input_graph`:

.. code-block:: python

    # print the input graph
    hidet.torch.dynamo_config.print_input_graph(True)

Because ResNet18 is a neat model without control flow, we can print the input graph to see how PyTorch dynamo
dispatches the model to hidet backend:

.. GENERATED FROM PYTHON SOURCE LINES 154-160

.. code-block:: default



    with torch.no_grad():
        hidet.torch.dynamo_config.print_input_graph(True)
        model_opt = torch.compile(model, backend='hidet')
        model_opt(x)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    opcode         name                        target                                                      args                                             kwargs
    -------------  --------------------------  ----------------------------------------------------------  -----------------------------------------------  --------
    placeholder    x                           x                                                           ()                                               {}
    call_module    self_conv1                  self_conv1                                                  (x,)                                             {}
    call_module    self_bn1                    self_bn1                                                    (self_conv1,)                                    {}
    call_module    self_relu                   self_relu                                                   (self_bn1,)                                      {}
    call_module    self_maxpool                self_maxpool                                                (self_relu,)                                     {}
    call_module    self_layer1_0_conv1         self_layer1_0_conv1                                         (self_maxpool,)                                  {}
    call_module    self_layer1_0_bn1           self_layer1_0_bn1                                           (self_layer1_0_conv1,)                           {}
    call_module    self_layer1_0_relu          self_layer1_0_relu                                          (self_layer1_0_bn1,)                             {}
    call_module    self_layer1_0_conv2         self_layer1_0_conv2                                         (self_layer1_0_relu,)                            {}
    call_module    self_layer1_0_bn2           self_layer1_0_bn2                                           (self_layer1_0_conv2,)                           {}
    call_function  iadd                        <built-in function iadd>                                    (self_layer1_0_bn2, self_maxpool)                {}
    call_module    self_layer1_0_relu_1        self_layer1_0_relu                                          (iadd,)                                          {}
    call_module    self_layer1_1_conv1         self_layer1_1_conv1                                         (self_layer1_0_relu_1,)                          {}
    call_module    self_layer1_1_bn1           self_layer1_1_bn1                                           (self_layer1_1_conv1,)                           {}
    call_module    self_layer1_1_relu          self_layer1_1_relu                                          (self_layer1_1_bn1,)                             {}
    call_module    self_layer1_1_conv2         self_layer1_1_conv2                                         (self_layer1_1_relu,)                            {}
    call_module    self_layer1_1_bn2           self_layer1_1_bn2                                           (self_layer1_1_conv2,)                           {}
    call_function  iadd_1                      <built-in function iadd>                                    (self_layer1_1_bn2, self_layer1_0_relu_1)        {}
    call_module    self_layer1_1_relu_1        self_layer1_1_relu                                          (iadd_1,)                                        {}
    call_module    self_layer2_0_conv1         self_layer2_0_conv1                                         (self_layer1_1_relu_1,)                          {}
    call_module    self_layer2_0_bn1           self_layer2_0_bn1                                           (self_layer2_0_conv1,)                           {}
    call_module    self_layer2_0_relu          self_layer2_0_relu                                          (self_layer2_0_bn1,)                             {}
    call_module    self_layer2_0_conv2         self_layer2_0_conv2                                         (self_layer2_0_relu,)                            {}
    call_module    self_layer2_0_bn2           self_layer2_0_bn2                                           (self_layer2_0_conv2,)                           {}
    call_module    self_layer2_0_downsample_0  self_layer2_0_downsample_0                                  (self_layer1_1_relu_1,)                          {}
    call_module    self_layer2_0_downsample_1  self_layer2_0_downsample_1                                  (self_layer2_0_downsample_0,)                    {}
    call_function  iadd_2                      <built-in function iadd>                                    (self_layer2_0_bn2, self_layer2_0_downsample_1)  {}
    call_module    self_layer2_0_relu_1        self_layer2_0_relu                                          (iadd_2,)                                        {}
    call_module    self_layer2_1_conv1         self_layer2_1_conv1                                         (self_layer2_0_relu_1,)                          {}
    call_module    self_layer2_1_bn1           self_layer2_1_bn1                                           (self_layer2_1_conv1,)                           {}
    call_module    self_layer2_1_relu          self_layer2_1_relu                                          (self_layer2_1_bn1,)                             {}
    call_module    self_layer2_1_conv2         self_layer2_1_conv2                                         (self_layer2_1_relu,)                            {}
    call_module    self_layer2_1_bn2           self_layer2_1_bn2                                           (self_layer2_1_conv2,)                           {}
    call_function  iadd_3                      <built-in function iadd>                                    (self_layer2_1_bn2, self_layer2_0_relu_1)        {}
    call_module    self_layer2_1_relu_1        self_layer2_1_relu                                          (iadd_3,)                                        {}
    call_module    self_layer3_0_conv1         self_layer3_0_conv1                                         (self_layer2_1_relu_1,)                          {}
    call_module    self_layer3_0_bn1           self_layer3_0_bn1                                           (self_layer3_0_conv1,)                           {}
    call_module    self_layer3_0_relu          self_layer3_0_relu                                          (self_layer3_0_bn1,)                             {}
    call_module    self_layer3_0_conv2         self_layer3_0_conv2                                         (self_layer3_0_relu,)                            {}
    call_module    self_layer3_0_bn2           self_layer3_0_bn2                                           (self_layer3_0_conv2,)                           {}
    call_module    self_layer3_0_downsample_0  self_layer3_0_downsample_0                                  (self_layer2_1_relu_1,)                          {}
    call_module    self_layer3_0_downsample_1  self_layer3_0_downsample_1                                  (self_layer3_0_downsample_0,)                    {}
    call_function  iadd_4                      <built-in function iadd>                                    (self_layer3_0_bn2, self_layer3_0_downsample_1)  {}
    call_module    self_layer3_0_relu_1        self_layer3_0_relu                                          (iadd_4,)                                        {}
    call_module    self_layer3_1_conv1         self_layer3_1_conv1                                         (self_layer3_0_relu_1,)                          {}
    call_module    self_layer3_1_bn1           self_layer3_1_bn1                                           (self_layer3_1_conv1,)                           {}
    call_module    self_layer3_1_relu          self_layer3_1_relu                                          (self_layer3_1_bn1,)                             {}
    call_module    self_layer3_1_conv2         self_layer3_1_conv2                                         (self_layer3_1_relu,)                            {}
    call_module    self_layer3_1_bn2           self_layer3_1_bn2                                           (self_layer3_1_conv2,)                           {}
    call_function  iadd_5                      <built-in function iadd>                                    (self_layer3_1_bn2, self_layer3_0_relu_1)        {}
    call_module    self_layer3_1_relu_1        self_layer3_1_relu                                          (iadd_5,)                                        {}
    call_module    self_layer4_0_conv1         self_layer4_0_conv1                                         (self_layer3_1_relu_1,)                          {}
    call_module    self_layer4_0_bn1           self_layer4_0_bn1                                           (self_layer4_0_conv1,)                           {}
    call_module    self_layer4_0_relu          self_layer4_0_relu                                          (self_layer4_0_bn1,)                             {}
    call_module    self_layer4_0_conv2         self_layer4_0_conv2                                         (self_layer4_0_relu,)                            {}
    call_module    self_layer4_0_bn2           self_layer4_0_bn2                                           (self_layer4_0_conv2,)                           {}
    call_module    self_layer4_0_downsample_0  self_layer4_0_downsample_0                                  (self_layer3_1_relu_1,)                          {}
    call_module    self_layer4_0_downsample_1  self_layer4_0_downsample_1                                  (self_layer4_0_downsample_0,)                    {}
    call_function  iadd_6                      <built-in function iadd>                                    (self_layer4_0_bn2, self_layer4_0_downsample_1)  {}
    call_module    self_layer4_0_relu_1        self_layer4_0_relu                                          (iadd_6,)                                        {}
    call_module    self_layer4_1_conv1         self_layer4_1_conv1                                         (self_layer4_0_relu_1,)                          {}
    call_module    self_layer4_1_bn1           self_layer4_1_bn1                                           (self_layer4_1_conv1,)                           {}
    call_module    self_layer4_1_relu          self_layer4_1_relu                                          (self_layer4_1_bn1,)                             {}
    call_module    self_layer4_1_conv2         self_layer4_1_conv2                                         (self_layer4_1_relu,)                            {}
    call_module    self_layer4_1_bn2           self_layer4_1_bn2                                           (self_layer4_1_conv2,)                           {}
    call_function  iadd_7                      <built-in function iadd>                                    (self_layer4_1_bn2, self_layer4_0_relu_1)        {}
    call_module    self_layer4_1_relu_1        self_layer4_1_relu                                          (iadd_7,)                                        {}
    call_module    self_avgpool                self_avgpool                                                (self_layer4_1_relu_1,)                          {}
    call_function  flatten                     <built-in method flatten of type object at 0x7f1f8ee00820>  (self_avgpool, 1)                                {}
    call_module    self_fc                     self_fc                                                     (flatten,)                                       {}
    output         output                      output                                                      ((self_fc,),)                                    {}





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  50.716 seconds)


.. _sphx_glr_download_gallery_tutorials_optimize-pytorch-model.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: optimize-pytorch-model.py <optimize-pytorch-model.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: optimize-pytorch-model.ipynb <optimize-pytorch-model.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
