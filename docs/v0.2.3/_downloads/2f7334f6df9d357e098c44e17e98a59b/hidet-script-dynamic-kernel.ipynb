{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Writing Dynamic kernel\n\n.. todo::\n\n  More details about hidet script and how to write dynamic kernel are coming soon.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy.testing\nimport hidet\n\n\ndef matmul_simt_kernel():\n    from hidet.lang import attr\n    from hidet.lang import float32, int32\n    from hidet.lang import as_tensor_pointer, tensor\n    from hidet.lang.cuda import threadIdx, blockIdx, syncthreads\n    from hidet.lang.mapping import repeat, spatial, auto_map\n    from hidet.lang.layout import row_layout, local_layout\n\n    warps_m, warps_n = 4, 2  # we use 4x2 warps\n    warp_m, warp_n = 2, 2  # each warp repeats 2x2 times\n    warp_map_m, warp_map_n = 2, 16  # each warp has 2x16 threads\n    thread_m, thread_n = 4, 4  # each thread repeats 4x4 times\n\n    # block_size = (64, 256, 8)\n    block_m_size, block_n_size = (\n        warps_m * warp_m * warp_map_m * thread_m,\n        warps_n * warp_n * warp_map_n * thread_n,\n    )\n    block_k_size = 8\n    num_warps = warps_m * warps_n  # 8\n    num_threads = num_warps * 32  # 256\n\n    with hidet.lang.script_module() as script_module:\n\n        @hidet.lang.script\n        def matmul_kernel(\n            a_ptr: ~float32,  # ~ means \"pointer to\", similar to \"*\" in C\n            b_ptr: ~float32,\n            c_ptr: ~float32,\n            m_size: int32,\n            n_size: int32,\n            k_size: int32,\n        ):\n            attr.func_name = 'matmul_kernel'\n            attr.cuda_block_dim = num_threads\n            attr.cuda_grid_dim = (\n                (m_size + block_m_size - 1) // block_m_size,\n                (n_size + block_n_size - 1) // block_n_size,\n            )\n\n            a = as_tensor_pointer(a_ptr, float32, [m_size, k_size])\n            b = as_tensor_pointer(b_ptr, float32, [k_size, n_size])\n            c = as_tensor_pointer(c_ptr, float32, [m_size, n_size])\n\n            smem_a = tensor('shared', float32, shape=[block_m_size, block_k_size])\n            smem_b = tensor('shared', float32, shape=[block_k_size, block_n_size])\n            regs_c = tensor(\n                scope='register',\n                dtype=float32,\n                # shape will be inferred from the layout automatically,\n                # in this case, the shape is [64, 256]\n                layout=(\n                    local_layout(warps_m, warps_n)\n                    * row_layout(warp_m, warp_n)\n                    * local_layout(warp_map_m, warp_map_n)\n                    * row_layout(thread_m, thread_n)\n                ),\n            )\n\n            # initialize the registers\n            mma_mapping = (\n                spatial(warps_m, warps_n)\n                .repeat(warp_m, warp_n)\n                .spatial(warp_map_m, warp_map_n)\n                .repeat(thread_m, thread_n)\n            )\n            for i, j in mma_mapping.on(threadIdx.x):\n                regs_c[i, j] = 0.0\n\n            # iterate over the k tiles\n            num_k_tiles = (k_size + block_k_size - 1) // block_k_size\n            for k_tile in range(num_k_tiles):\n\n                # load smem_a [block_m_size, block_k_size] from global memory\n                for i, k in auto_map(block_m_size, block_k_size, workers=num_threads).on(\n                    threadIdx.x\n                ):\n                    global_i, global_k = (\n                        i + blockIdx.x * block_m_size,\n                        k + k_tile * block_k_size,\n                    )\n                    smem_a[i, k] = (\n                        a[global_i, global_k]\n                        if global_i < m_size and global_k < k_size\n                        else 0.0\n                    )\n\n                # load smem_b [block_k_size, block_n_size] from global memory\n                for k, j in auto_map(block_k_size, block_n_size, workers=num_threads).on(\n                    threadIdx.x\n                ):\n                    global_k, global_j = (\n                        k + k_tile * block_k_size,\n                        j + blockIdx.y * block_n_size,\n                    )\n                    smem_b[k, j] = (\n                        b[global_k, global_j]\n                        if global_k < k_size and global_j < n_size\n                        else 0.0\n                    )\n\n                # synchronize all threads in the block\n                syncthreads()\n\n                # simt matrix multiply accumulate (mma): regs_c = regs_c + smem_a @ smem_b\n                for i, j in mma_mapping.on(threadIdx.x):\n                    for k in range(block_k_size):\n                        regs_c[i, j] += smem_a[i, k] * smem_b[k, j]\n\n                # synchronize all threads in the block\n                syncthreads()\n\n            # store regs_c back to global memory\n            for i, j in mma_mapping.on(threadIdx.x):\n                global_i = i + blockIdx.x * block_m_size\n                global_j = j + blockIdx.y * block_n_size\n                if global_i < m_size and global_j < n_size:\n                    c[global_i, global_j] = regs_c[i, j]\n\n    assert isinstance(matmul_kernel, hidet.ir.Function)  # matmul is a hidet.ir.Function\n\n    ir_module = script_module.ir_module()\n    compiled_function: hidet.runtime.CompiledFunction = hidet.driver.build_ir_module(\n        ir_module\n    )\n    return compiled_function\n\n\ndef main():\n    func = matmul_simt_kernel()\n\n    for m, n, k in [(1024, 1024, 1024), (333, 444, 555), (1, 12, 13)]:\n        a = hidet.randn([m, k], dtype='float32').cuda()\n        b = hidet.randn([k, n], dtype='float32').cuda()\n        c = hidet.zeros([m, n]).cuda()\n        func(a, b, c, m, n, k)\n        numpy.testing.assert_allclose(\n            actual=c.cpu().numpy(),\n            desired=a.cpu().numpy() @ b.cpu().numpy(),\n            rtol=1e-4,\n            atol=1e-4,\n        )\n\n        hidet_latency = hidet.utils.benchmark_func(\n            lambda: func(a, b, c, m, n, k), repeat=50\n        )\n        print(f'{m}x{k}x{n}: hidet takes {hidet_latency:.2f} ms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}